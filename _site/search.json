[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer, If they are, then they will be launched into R.\n\npacman::p_load(tidyverse)\n\n\n\n\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "2.1 Overview\nThis chapter introduces several ggplot2 extensions for creating more elegant and effective statistical graphics. We will be able to:\n\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package,\ncreate professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages,\nplot composite figure by combining ggplot2 graphs by using patchwork package.\n\n\n\n2.2 Getting Started\n\nInstall and launch R packages\nThe code chunk below below will be used to check if these packages have been installed and also will load them onto the working R environment.\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2\n\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\n\n\nImporting Data\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package. readr is one of the tidyverse package.\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n2.3 Beyond ggplot2 Annotation: ggrepel\nOne of the challenge in plotting statistical graph is annotation, especially with large number of data points.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWorking with ggrepel\nggrepel is an extension of ggplot2 package which provides geoms for ggplot2 to repel overlapping text as in our examples on the right. We simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: ggrepel: 317 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n2.4 Beyond ggplot2 Themes\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others. In the example below, The Economist theme is used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\nWorking with hrbthems package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\nNote that:\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines.\n\n\n\n\n2.5 Beyond Single Graph\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose figure with multiple graphs. In this section, we learn how to create composite plot by combining multiple graphs. First, let us create three statistical graphics by using the code chunk below.\nhrbrthemes::import_roboto_condensed()\n\np1 <- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\nNext,\n\np2 <- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\nLastly, we will draw a scatterplot for English score versus Maths score as shown below:\n\np3 <- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\nCreating Composite Graphics: patchwork methods\nThere are several ggplot2 extension’s functions support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package. In this section, an ggplot2 extension called patchwork which is specially designed for combining separate ggplot2 graphs into a single figure will be used.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines: - Two-Column Layout using the Plus Sign +. - Parenthesis () to create a subplot group. - Two-Row Layout using the Division Sign /\n\n\nCombining two ggplot2 graphs\nFigure in the tabset below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot!\n\np1 + p2\n\n\n\n\n\n\nCombining three ggplot2 graph\nWe can plot more complex composite by using appropriate operators. For example, the composite figure below is plotted by using: - “|” operator to stack two ggplot2 graphs, - “/” operator to place the plots beside each other, - “()” operator the define the sequence of the plotting.\n\n(p1 / p2) | p3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCreating a composite figure with tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below.\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCreating figure with insert\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\np12 <- p1|p2\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCreating a composite figure by using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\npatchwork <- (p1 / p2) | p3\npatchwork & theme_economist()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "3.0 Programming Interactive Data Visualisation with R\n\n3.1 Overview\nThis is to create interactive data visualisation by using functions provided by ggiraph and plotlyr packages\n\n\n3.2 Getting Started\nThe following code chunk installs and launchs the following R packages:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse) \n\n\n\n3.3 Importing Data\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n3.4 Interactive Data Visualisation - ggigraph methods\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements. Onclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked. Data_id: a column of data-sets that contain an id to be associated with elements. If it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation\n\n3.4.1 Tooltip effect with tooktip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nNotice that two steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\n\n\n3.4.2 Displaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\n\nexam_data$tooltip <- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n3.4.3 Customising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css <- \"background-color:white; #<<\nfont-style:bold; color:black;\" #<<\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #<<\n    opts_tooltip(    #<<\n      css = tooltip_css)) #<<\n)                                        \n\n\n\n\n\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\n\n\n3.4.4 Displaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip <- function(y, ymax, accuracy = .01) {\n  mean <- scales::number(y, accuracy = accuracy)\n  sem <- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point <- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n3.4.5 Hover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\nNote that the default value of the hover css is hover_css = “fill:orange;”.\n\n\n3.4.6 Styling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\n3.4.7 Combining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n3.4.8 Click effect with oneclick\nonclick argument of ggiraph provides hotlink interactivity on the web. The code chunk below shown an example of onclick.\n\nexam_data$onclick <- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\nNote that click actions must be a string column in the dataset containing valid javascript instructions.\n\n\n3.4.9 Coordinated Multiple Views with ggiraph\n\np1 <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 <- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\nCoordinated multiple views methods has been implemented in the data visualisation.\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point.\n\n\n\n3.5 Interactive Data Visualisation - plotly methods!\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n3.5.1 Creating an interactive scatter plot:plot_ly() method\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n3.5.2 Working with visual variable:plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n3.5.3 Creating an interactive scatter plot:ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\np <- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n3.5.4 Coordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nd <- highlight_key(exam_data)\np1 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\nThing to learn from the code chunk:\nhighlight_key() simply creates an object of class crosstalk::SharedData. Visit this link to learn more about crosstalk.\n\n\n\n3.6 Interactive Data Visualisation - crosstalk methods!\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n3.6.1 Interactive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n3.6.2 Linked brushing:crosstalk method\nCode chunk below is used to implement the coordinated brushing shown above.\n\nd <- highlight_key(exam_data) \np <- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nSetting the `off` event (i.e., 'plotly_deselect') to match the `on` event (i.e., 'plotly_selected'). You can change this default via the `highlight()` function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!.\n\n\n\n\n3.7 Reference\n\n3.7.1 ggiraph\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\n3.7.2 plotly for R\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels\n\n\n\n\n\n4.0 Programming Animated Statistical Graphics with R\n\n4.1 Overview\nWhen telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. IWe will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, we will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\n4.1.1 Basic concepts of animation\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n4.1.2 Terminology\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n4.2 Getting Started\n\n4.2.1 Loading the R packages\nThe code chunk below will check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n4.2.2 Importing the data\nThe Data worksheet from GlobalPopulation Excel workbook will be used.\nThe code chunk below imports Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol <- c(\"Country\", \"Continent\")\nglobalPop <- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %>%\n  mutate_each_(funs(factor(.)), col) %>%\n  mutate(Year = as.integer(Year))\n\nWarning: `mutate_each_()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nThings to learn from the code chunk above:\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\n\n\n\n\n4.3 Animated Data Visualisation:gganimate methods\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_()/exit_() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n4.3.1 Building a static population bubble plot\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n4.3.2 Building the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')          \n\n\n\n\n\n\n\n4.4 Animated Data Visualisation: plotly\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n4.4.1 Building an animated bubble plot: ggplotly() method\nIn this sub-section, we will learn how to create an animated bubble plot by using ggplotly() method.\n\ngg <- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nWarning in geom_point(aes(size = Population, frame = Year), alpha = 0.7, :\nIgnoring unknown aesthetics: frame\n\nggplotly(gg)\n\nWarning in p$x$data[firstFrame] <- p$x$frames[[1]]$data: number of items to\nreplace is not a multiple of replacement length\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\n4.4.2 Building an animated bubble plot: plot_ly() method\nIn this sub-section, we will learn how to create an animated bubble plot by using plot_ly() method.\n\nbp <- globalPop %>%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent, \n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          )\nbp\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "9.0 Visual Statistical Analysis\n\n9.1 Learning Outcome\nThe following are the learning outcome:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters\n\n\n\n9.2 Visual Statistical Anaysis with ggstatplot\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.It provides alternative statistical inference methods by default. To follow best practices for statistical reporting.\n\n\n9.3 Getting Started\n\n9.3.1 Installing and launching R packages\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n9.3.2 Importing Data\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\n\n\n9.3.3 One-sample test: gghistostats() method\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n9.3.4 Unpacking the Bayes Factor\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10.\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n9.3.5 How to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013.\n\n\n9.3.6 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam_data,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n9.3.7 Oneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam_data,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n-“ns” → only non-significant\n-“s” → only significant -“all” → everything\n\n9.3.7.1 ggbetweenstats - Summary of tests\n\n\n\n9.3.8 Significant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam_data,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n9.3.9 Significant Test of Association (Dependence): ggbarstats() methods\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 <- exam_data %>% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n9.4 Visualising Models\nIn this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\n9.5 Getting Started\n\n\n9.6 Installing and loading the required libraries\n\n\n\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n9.6.1 Importing Excel file: readxl methods\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale <- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   <dbl> <chr>    <dbl>     <dbl>     <dbl>    <dbl>  <dbl>         <dbl>  <dbl>\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period <dbl>, HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>, …\n\n\n\n\n9.6.2 Multiple Regresssion Model using Im()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n9.6.3 Model Diagnostic: checking for multicollinearity\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c <- check_collinearity(model)\nplot(check_c)\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n9.6.4 Model Diagnostic: checking normality assumption\nIn the code chunk, check_normality() of performance package.\n\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n <- check_normality(model1)\n\n\nplot(check_n)\n\n\n\n\n\n\n9.6.5 Model Diagnostic: check model for homogeneity of variances\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h <- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\n\n\n9.6.6 Model Diagnostic: complete check\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n9.6.7 Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n9.6.8 Visualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")\n\n\n\n\n\n\n\n\n10. Visualising Uncertainty\n\n10.1 Learning Outcome\n\n\n10.2 Visualizing the Uncertainty of point estimates\n\nA point estimate is a single number, such as a mean.\nUncertainty is expressed as standard error, confidence interval, or credible interval\nImportant:Don’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\npacman::p_load(tidyverse, plotly, crosstalk, DT, ggdist, gganimate)\n\n\nexam <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n10.2.1 Visualizing the uncertainty of point estimates: ggplot2 methods\nThe code chunk below performs the followings:\n\ngroup the observation by RACE,\ncomputes the count of observations, mean, standard deviation and standard error of Maths by RACE, and\nsave the output as a tibble data table called my_sum.\n\n\nmy_sum <- exam %>%\n  group_by(RACE) %>%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n \n  \n    RACE \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    Chinese \n    193 \n    76.50777 \n    15.69040 \n    1.132357 \n  \n  \n    Indian \n    12 \n    60.66667 \n    23.35237 \n    7.041005 \n  \n  \n    Malay \n    108 \n    57.44444 \n    21.13478 \n    2.043177 \n  \n  \n    Others \n    9 \n    69.66667 \n    10.72381 \n    3.791438 \n  \n\n\n\n\n\n\n\n10.2.2 Visualizing the uncertainty of point estimates: ggplot2 methods\nThe code chunk below is used to reveal the standard error of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean \n          maths score by race\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n10.2.3 Visualizing the uncertainty of point estimates: ggplot2 methods\nPlot the 95% confidence interval of mean maths score by race. The error bars should be sorted by the average maths scores.\n\nmy_sum$RACE = with(my_sum, reorder(RACE,-mean))\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"95% cobfidence interval of mean \n          maths score by race\")\n\n\n\n\n\n\n10.2.4 Visualising the uncertainty of point estimates with interactive error bars\nPlot interactive error bars for the 99% confidence interval of mean maths score by race.\n\nMy_sum2 <- rename(my_sum, c(\"No. of pupils\" = n, \"Ave Scores\" = mean, \"Std Dev\" = sd, \"Std Error\" = se))\n\nMy_sum2\n\n# A tibble: 4 × 5\n  RACE    `No. of pupils` `Ave Scores` `Std Dev` `Std Error`\n  <fct>             <int>        <dbl>     <dbl>       <dbl>\n1 Chinese             193         76.5      15.7        1.13\n2 Indian               12         60.7      23.4        7.04\n3 Malay               108         57.4      21.1        2.04\n4 Others                9         69.7      10.7        3.79\n\n\n\nDT::datatable(My_sum2, class= \"compact\")\n\n\n\n\n\n\n`\n\nd <- highlight_key(My_sum2) \np <- ggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-2.58*se, \n        ymax=mean+2.58*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"95% cobfidence interval of mean \n          maths score by race\")\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3 Visualising Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n10.3.1 Visualising the certainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +   #<<\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nGentle advice: This function comes with many arguments, it is advised to read the syntax reference for more detail.\n\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\nWarning in layer_slabinterval(data = data, mapping = mapping, stat =\nStatPointinterval, : Ignoring unknown parameters: `.point` and `.interval`\n\n\n\n\n\n\n\n10.3.2 Visualising the certainty of point estimates: ggdist methods\nMakeover the plot on previous slide by showing 95% and 99% confidence intervals.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n10.3.3 Visualising the certainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\nGentle advice: This function comes with many arguments, it is advised to read the syntax reference for more details.\n\n\n\n10.4 Visualisaing Uncertainty with Hypothetical Outcome Plots (HOPs)\nStep 1: Installing ungeviz package\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nSkipping install of 'ungeviz' from a github remote, the SHA1 (aeae12b0) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\nNote: Only need to perform this step once.\nStep 2: Launch the application in R\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n10.5 Visualisaing Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n\n11. Funnel Plots for Fair Comparisons\n\n11.1 Overview\nFunnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages.\n\n\n\n11.2 Installing and Launching R Packages\nFive R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n11.3 Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. We are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 <- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %>%\n  mutate_if(is.character, as.factor)\n\nRows: 267 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): City, District, Sub-district\ndbl (4): Sub-district ID, Positive, Recovered, Death\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n11.4 FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n11.4.1 FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n11.4.2 FunnelPlotR methods: Makeover 1\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #<<\n  xrange = c(0, 6500),  #<<\n  yrange = c(0, 0.05)   #<<\n)\n\nWarning: The `xrange` argument deprecated; please use the `x_range` argument\ninstead. For more options, see the help: `?funnel_plot`\n\n\nWarning: The `yrange` argument deprecated; please use the `y_range` argument\ninstead. For more options, see the help: `?funnel_plot`\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n11.4.3 FunnelPlotR methods: Makeover 2\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #<<           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #<<\n  y_label = \"Cumulative Fatality Rate\"  #<<\n)\n\nWarning: The `xrange` argument deprecated; please use the `x_range` argument\ninstead. For more options, see the help: `?funnel_plot`\n\n\nWarning: The `yrange` argument deprecated; please use the `y_range` argument\ninstead. For more options, see the help: `?funnel_plot`\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles.\n\n\n\n\n11.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, we can gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n11.5.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf <- covid19 %>%\n  mutate(rate = Death / Positive) %>%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %>%\n  filter(rate > 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean <- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n11.5.2 Calculate lower and upper limits for 95% and 99.9% CI\n\nnumber.seq <- seq(1, max(df$Positive), 1)\nnumber.ll95 <- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 <- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 <- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 <- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI <- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n11.5.3 Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np <- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n11.5.4 Interactive Funnel Plot: Plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly <- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly\n\n\n\n\n\n\n\n\n\n\n10.2.3 Visualizing the uncertainty of point estimates: ggplot2 methods\nPlot the 95% confidence interval of mean maths score by race. The error bars should be sorted by the average maths scores.\n\nmy_sum$RACE = with(my_sum, reorder(RACE,-mean))\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"95% cobfidence interval of mean \n          maths score by race\")\n\n\n\n\n\n\n10.2.4 Visualising the uncertainty of point estimates with interactive error bars\nPlot interactive error bars for the 99% confidence interval of mean maths score by race.\n\nMy_sum2 <- rename(my_sum, c(\"No. of pupils\" = n, \"Ave Scores\" = mean, \"Std Dev\" = sd, \"Std Error\" = se))\n\nMy_sum2\n\n# A tibble: 4 × 5\n  RACE    `No. of pupils` `Ave Scores` `Std Dev` `Std Error`\n  <fct>             <int>        <dbl>     <dbl>       <dbl>\n1 Chinese             193         76.5      15.7        1.13\n2 Indian               12         60.7      23.4        7.04\n3 Malay               108         57.4      21.1        2.04\n4 Others                9         69.7      10.7        3.79\n\n\n\nDT::datatable(My_sum2, class= \"compact\")\n\n\n\n\n\n\n`\n\nd <- highlight_key(My_sum2) \np <- ggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-2.58*se, \n        ymax=mean+2.58*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"95% cobfidence interval of mean \n          maths score by race\")\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Visualising the certainty of point estimates: ggdist methods\nMakeover the plot on previous slide by showing 95% and 99% confidence intervals.\n\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n                     .point = median,\n                     .interval = \"confidence\",\n                     level = 0.95) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot (95% confidence intervals)\"\n  )\n\nWarning in layer_slabinterval(data = data, mapping = mapping, stat =\nStatPointinterval, : Ignoring unknown parameters: `.point`, `.interval`, and\n`level`\n\n\n\n\n\n\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n                     .point = median,\n                     .interval = \"confidence\",\n                     level = 0.99) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot (99% confidence intervals)\"\n  )\n\nWarning in layer_slabinterval(data = data, mapping = mapping, stat =\nStatPointinterval, : Ignoring unknown parameters: `.point`, `.interval`, and\n`level`"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 5",
    "section": "",
    "text": "Modelling, Visualising and Analysing Network Data with R\n\n\n27.1 Overview\nThis exercise teaches us to model, analyse and visualise network data using R. By the end exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package.\n\n\n\n27.2 Getting Started\n\n27.2.1 Installing and launching R packages\nFour network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n27.3 The Data\nThe data sets used is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n27.3.1 The Edges data\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n27.3.2 The nodes data\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n27.3.3 Importing network data from files\nWe will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n27.3.4 Reviewing the imported data\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! It is important for us to change the data type of SentDate field back to “Date”” data type\n\n\n\n\n27.3.5 Wrangling time\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n27.3.6 Reviewing the revised date fields\nTable below shows the data structure of the reformatted GAStech_edges data frame.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     <ord> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n27.3.7 Wrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation. In view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n27.3.8 Reviewing revised edges file\nTable below shows the data structure of the reformatted GAStech_edges data frame.\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday <ord> Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  <int> 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…\n\n\n\n\n\n27.4 Creating network objects using tidygraph\nIn this section, we will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\n\n27.4.1 The tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n27.4.2 The dplyr verbs in tidygraph\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n27.4.3 Using tbl_graph() to build tidygraph data model\nIn this section, we will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame. Before typing the codes, it is recommended to review to reference guide of tbl_graph()\n\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n27.4.4 Reviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,372 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n27.4.5 Reviewing the output tidygraph’s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n27.4.6 Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\nGAStech_graph %>%\n  activate(edges) %>%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 1,372 × 4\n   from    to Weekday  Weight\n  <int> <int> <ord>     <int>\n1    40    41 Saturday     13\n2    41    43 Monday       11\n3    35    31 Tuesday      10\n4    40    41 Monday       10\n5    40    43 Monday       10\n6    36    32 Sunday        9\n# ℹ 1,366 more rows\n#\n# A tibble: 54 × 4\n     id label           Department     Title           \n  <dbl> <chr>           <chr>          <chr>           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\n\n\n27.5 Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs. As in all network graph, there are three main aspects to a ggraph’s network graph, they are: nodes, edges and layouts.\n\n27.5.1 Plotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\n\nCode\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n:::callout-Things to learn from code chunk above: The basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object. :::\n\n\n27.5.2 Changing the default network graph theme\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\n\nCode\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n:::callout-Things to learn from code chunk above: - ggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\n\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots. :::\n\n\n\n27.5.3 Changing the colour of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\n\nCode\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n27.5.4 Working with ggraph’s layout\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n27.5.5 Fruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\n\nCode\ng <- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\nThing to learn from the code chunk above - layout argument is used to define the layout to be used.\n\n\n27.5.6 Modifying network nodes\nIn this section, you will colour each node by referring to their respective departments.\n\n\nCode\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n27.5.7 Modifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\n\nCode\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\n\n27.6 Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n27.6.1 Working with facet_edge()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nCode\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n27.6.2 Working with facet_edges()\nThe code chunk below uses theme() to change the position of the legend.\n\n\nCode\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n27.6.3 A framed facet graph\nThe code chunk below adds frame to each graph.\n\n\nCode\nset_graph_style() \n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n27.6.4 Working with facet_nodes()\nIn the code chunk below, facet_nodes() is used.\n\n\nCode\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n27.7 Network Metrics Analysis\n\n27.7.1 Computing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector.\n\n\nCode\ng <- GAStech_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation. the algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n27.7.2 Visualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\n\nCode\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n27.7.3 Visualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph.\nIn the code chunk below group_edge_betweenness() is used.\n\n\nCode\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n27.8 Building interactive network graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nWe can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nWe can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n27.8.1 Data preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\n\nCode\nGAStech_edges_aggregated <- GAStech_edges %>%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %>%\n  rename(from = id) %>%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %>%\n  rename(to = id) %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 1) %>%\n  ungroup()\n\n\n\n\n27.8.2 Plotting the first interactive network graph using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n27.8.3 Working with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\n\nCode\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n\n27.8.4 Working with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\n\nCode\nGAStech_nodes <- GAStech_nodes %>%\n  rename(group = Department) \n\n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\n\nCode\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n27.8.5 Working with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges. - The argument arrows is used to define where to place the arrow. - The smooth argument is used to plot the edges using a smooth curve.\n\n\nCode\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n27.8.6 Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\n\nCode\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class_Ex01",
    "section": "",
    "text": "Getting Started\n\nUsing p_load() of pacman package to load tidyverse\nThe code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer, If they are, then they will be launched into R.\n\npacman::p_load(tidyverse)\n\n\n\nImporting the data\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nWorking with Theme\nThe code chunk below is to plot a horizontal bar chart - Changing the colors of plot panel background of theme_minimal() to light blue and the color of grid lines to white.\n\nggplot(data=exam_data, aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"lightblue\", colour = \"lightblue\", size = 0.5, linetype = \"solid\"),\n    panel.grid.major = element_line(size = 0.5, linetype = 'solid', colour = \"white\"), \n    panel.grid.minor = element_line(size = 0.25, linetype = 'solid', colour = \"white\"))\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\nDesigning Data-driven Graphics for Analysis I\nThis code chunk is to improve the following: - y-aixs label is not clear (i.e. count) - To support effective comparison, the bars should be sorted by their resepctive frequencies. - For static graph, frequency values should be added to provide addition information.\n\nggplot(data=exam_data, \n       aes(x=reorder(RACE,RACE,\n                     function(x)-length(x)))) +\n  geom_bar() +\n  ylim(0,220) +\n  geom_text(stat=\"count\", \n      aes(label=paste0(..count.., \", \", \n      round(..count../sum(..count..)*100, 1), \"%\")),\n      vjust=-1) +\n  xlab(\"Race\") +\n  ylab(\"No. of\\nPupils\") +\n  theme(axis.title.y=element_text(angle = 0))\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nThis code chunk uses fct_infreq() of forcats package.\n\nexam_data %>%\n  mutate(RACE = fct_infreq(RACE)) %>%\n  ggplot(aes(x = RACE)) + \n  geom_bar()+\n  ylim(0,220) +\n  geom_text(stat=\"count\", \n      aes(label=paste0(..count.., \", \", \n      round(..count../sum(..count..)*100,\n            1), \"%\")),\n      vjust=-1) +\n  xlab(\"Race\") +\n  ylab(\"No. of\\nPupils\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n\n\n\n\nDesigning Data-driven Graphics for Analysis II\nThe code chunk below does the following improvements: - Adding mean and median lines on the histogram plot. - Change fill color and line colour.\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(MATHS, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=1) +\n  geom_vline(aes(xintercept=median(MATHS, na.rm=T)),\n             color=\"grey30\",\n             linetype=\"dashed\", \n             size=1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nDesigning Data-driven Graphics for Analysis III\nThe code chunk below shows the background having the distribution of English scores for all pupils.\n\nd <- exam_data   \nd_bg <- d[, -3]  \n\nggplot(d, aes(x = ENGLISH, fill = GENDER)) +\n  geom_histogram(data = d_bg, fill = \"grey\", alpha = .5) +\n  geom_histogram(colour = \"black\") +\n  facet_wrap(~ GENDER) +\n  guides(fill = FALSE) +  \n  theme_bw()\n\nWarning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nDesigning Data-driven Graphics for Analysis IV\nThe code chunk below shows the scatterplot of English vs math scores for all students.\n\nggplot(data=exam_data, \n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point() +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  geom_hline(yintercept=50,\n             linetype=\"dashed\",\n             color=\"grey60\",\n             size=1) + \n  geom_vline(xintercept=50, \n             linetype=\"dashed\",\n             color=\"grey60\",\n             size=1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-Class Exercise 4",
    "section": "",
    "text": "pacman::p_load(rstatix, gt, patchwork, tidyverse)\n\n\nexam_data <-read_csv(\"data/Exam_data.csv\")\n\nPlot Q-Q plot. Conversely, if the points deviate significantly from straight diagonal line, it is less likely that data is normally distributed.\n\nggplot(exam_data,\n       aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\nCreate a tab to toggle between code chunk and plot\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data,\n       aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\nCreate a call-out as notes.\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the points deviate significant from the straight diagonal line. This is a clear indication thatr the data is not normally distributed.\n\n\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\nqq <- ggplot(exam_data,\n       aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line()\n  \nsw_t <- exam_data %>%\n   shapiro_test(ENGLISH) %>%\n   gt()\n \ntmp <- tempfile(fileext = '.png')\ngtsave(sw_t, tmp)\ntable_png <- png::readPNG(tmp, native= TRUE)\n \nqq + table_png"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class_Ex05",
    "section": "",
    "text": "Overview\nIn this hands-on exercise, you will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, you will be able to:\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph, build network graph visualisation using appropriate functions of ggraph, compute network geometrics using tidygraph, build advanced graph visualisation by incorporating the network geometrics, and build interactive network visualisation using visNetwork package.\nGetting Started\nInstalling and launching R packages\nFour network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\nThe Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\nThe edges data GAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\nThe nodes data GAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\nImporting network data from files In this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\")\n\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     <ord> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\n27.4.3 Using tbl_graph() to build tidygraph data model.\n\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n27.4.4 Reviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,372 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n\n\n\n\npacman::p_load(jsonlite, igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\nMC1 <- fromJSON(\"data/MC1.json\")\n\n\nMC1_nodes <- as_tibble(MC1$nodes) %>%\n  select(id, type, country)\n\n\nMC1_edges  <- as_tibble(MC1$links) %>%\n  select(source, target, type, weight, key)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "title": "In-class Exercise 6",
    "section": "",
    "text": "Testing"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608_VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications! In this website, you will find all interesting content about this course!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take_home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take_home_Ex01.html",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "1. The Task\nCity of Engagement, with a total population of 50,000, is a small city located at Country of Nowhere. The city serves as a service centre of an agriculture region surrounding the city. The main agriculture of the region is fruit farms and vineyards. The local council of the city is in the process of preparing the Local Plan 2023. A sample survey of 1000 representative residents had been conducted to collect data related to their household demographic and spending patterns, among other things. The city aims to use the data to assist with their major community revitalization efforts, including how to allocate a very large city renewal grant they have recently received. This study will provide user-friendly and interactive solution that helps city managers and planners to explore the complex data in an engaging way and reveal hidden patterns.\n\n\n2. The Data\nTwo datasets are provided for this study:\n\nParticipants.csv\nThis Dataset contains information about the residents of City of Engagement that have agreed to participate in this study.\n\nparticipantId (integer): unique ID assigned to each participant.\nhouseholdSize (integer): the number of people in the participant’s household\nhaveKids (boolean): whether there are children living in the participant’s household.\nage (integer): participant’s age in years at the start of the study.\neducationLevel (string factor): the participant’s education level, one of: {“Low”, “HighSchoolOrCollege”, “Bachelors”, “Graduate”}\ninterestGroup (char): a char representing the participant’s stated primary interest group, one of {“A”, “B”, “C”, “D”, “E”, “F”, “G”, “H”, “I”, “J”}.\njoviality (float): a value ranging from [0,1] indicating the participant’s overall happiness level at the start of the study.\n\n\n\nFinancialJournal.csv\nThis dataset contains information about financial transactions.\n\nparticipantId (integer): unique ID corresponding to the participant affected\ntimestamp (datetime): the time when the check-in was logged\ncategory (string factor): a string describing the expense category, one of {“Education”, “Food”, “Recreation”, “RentAdjustment”, “Shelter”, “Wage”}\namount (double): the amount of the transaction\n\n\n\n\n3. Data Preparation\n\n3.1 Install and launch R packages\nThe code chunk below is used to install and load the required packages onto RStudio.\n\ntidyverse: a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\ndplyr: a package in R that provides a set of functions for data manipulation, transformation, and summarization\nggplot2: a system for creating graphics, based on The Grammar of Graphics\nggstatplot: an extension of ggplot2 package for creating graphics with details from statistical tests included in the plots themselves and targeted primarily at behavioral sciences community to provide a one-line code to produce information-rich plots.\nggrepel: a package provides geoms for ggplot2 to repel overlapping text labels.\nggiraph: a package that provides interactive elements to ggplot like animations and tooltips (was not used after experimenting with it, leaving it here for reference).\nggridges: a package for creating ridge plots, which are a type of data visualization that displays the distribution of a continuous variable for different categories.\nggthemes: a package provides some extra themes, geoms, and scales for ‘ggplot2’.\ngganimate: a package that allows for the creation of animated visualizations using ggplot2. It provides a framework for creating animated plots from a static ggplot object by mapping aesthetic attributes to time.\nggdist: a package that provides functions for generating simulated data from common distributions and for calculating and visualizing various summary statistics, such as posterior distributions from Bayesian models.\nhrbrthemes: a package provides typography-centric themes and theme components for ggplot2.\nplotly: another package that provides interactive elements to ggplot.\nungeviz: apackage that provides a collection of interactive visualizations for exploratory data analysis.\ntreemap: a package provides an easy way to create treemaps, which are visualizations that display hierarchical data as a set of nested rectangles\n\n\n\nCode\npacman::p_load(ggrepel, ggstatsplot, ggplot2, plotly, ggridges, ggdist, ungeviz, gganimate, performance, ggiraph, ggthemes, hrbrthemes, tidyverse, viridis, treemap, dplyr) \n\n\n\n\n3.2 Importing Data\nThe 2 datasets are imported into R environment.\n\n\nCode\npart <- read_csv(\"data/Participants.csv\")\n\n\n\n\nCode\nfj <- read_csv(\"data/FinancialJournal.csv\")\n\n\n\n\n3.3 Data Wrangling\n\n3.3.1 FinancialJournal.csv\nFirst, conversion of “timestamp” column to “Month-Year” format in chr form is carried out.\n\n\nCode\nfj$Month_Yr <- format(as.Date(fj$timestamp), \"%Y-%m\")\n\n\nA check for duplicate rows in the dataset is conducted. By running the code chunk below, it is found that there are 1113 duplicate rows. These duplicate rows are subsequently removed from the analysis.\n\n\nCode\n# Check for full duplicate rows\nduplicated_rows <- fj[duplicated(fj, fromLast = TRUE),]\n\n# Remove the duplicate rows (if any)\nif (nrow(duplicated_rows) > 0) {\n  print(\"Full duplicate rows found and removed:\")\n  fj1 <- subset(fj, !duplicated(fj, fromLast = TRUE))\n} else {\n  print(\"No full duplicate rows found.\")\n}\n\n\n[1] \"Full duplicate rows found and removed:\"\n\n\nThe dataset is pivoted to display the different categories of costs as separate columns.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Group by two columns (e.g., \"category\" and \"year\") and summarize the values in another column (e.g., \"value\")\nfj1_grouped <- fj1 %>%\n  group_by(participantId, Month_Yr, category) %>%\n  summarize(total_amount = sum(amount))\n\n# Pivot the data from rows to columns\nfj1_pivoted <- fj1_grouped %>%\n  pivot_wider(names_from = category, values_from = total_amount)\n\n\nAll “NA” values are replaced with “0” in all columns. A new column is created to compute new rent amount, taking into consideration of the rent adjustment.\n\n\nCode\n# Replace \"NA\" values in column with \"0\"\n#fj1_pivoted$RentAdjustment <- ifelse(is.na(fj1_pivoted$RentAdjustment), 0, fj1_pivoted$RentAdjustment)\nfj1_pivoted[is.na(fj1_pivoted)] <- 0.0\n\n# Add the values in column A to column B and store in a new column C\nfj1_pivoted$Shelter_rev <- fj1_pivoted$Shelter + fj1_pivoted$RentAdjustment\n\n\nUnder the expenditure columns, there are negative integers that indicates outlay of costs. All these values are adjusted to positive integers and rounded 2 decimal points, for clarity and consistency.\n\n\nCode\n# Convert the values in a column to positive integers, rounded to 2 decimal points\nfj1_pivoted$Education <- round(abs(fj1_pivoted$Education), 2)\nfj1_pivoted$Food <- round(abs(fj1_pivoted$Food), 2)\nfj1_pivoted$Recreation <- round(abs(fj1_pivoted$Recreation), 2)\nfj1_pivoted$Shelter <- round(abs(fj1_pivoted$Shelter), 2)\nfj1_pivoted$Wage <- round(abs(fj1_pivoted$Wage), 2)\nfj1_pivoted$RentAdjustment <- round(abs(fj1_pivoted$RentAdjustment), 2)\nfj1_pivoted$Shelter_rev <- round(abs(fj1_pivoted$Shelter_rev), 2)\n\n\nLastly, new columns for total income and total expenditures are created in the dataset to facilitate the subsequent analysis.\n\n\nCode\nfj1_pivoted$Total_income <- fj1_pivoted$Wage\n\n# creating new columns\nfj1_pivoted$Total_expenditure <- fj1_pivoted$Education + fj1_pivoted$Food + fj1_pivoted$Recreation + fj1_pivoted$Shelter_rev\n\n\n\n\n3.3.2 Participants.csv\nThe dataset contains 1011 (instead of 1000 as informed) representative participants. Check for duplicate rows in the dataset is carried out and there are no duplicates found in the data.\n\n\nCode\n# Check for full duplicate rows\nduplicated_rows <- part[duplicated(part, fromLast = TRUE),]\n\n# View the duplicate rows (if any)\nif (nrow(duplicated_rows) > 0) {\n  print(\"Full duplicate rows found:\")\n  print(subset(part, duplicated(part, fromLast = TRUE)))\n} else {\n  print(\"No full duplicate rows found.\")\n}\n\n\n[1] \"No full duplicate rows found.\"\n\n\nIn addition, the following data preparation is carried out: (i) Recode “Bachelors” to be same as “Graduate” under “educationLevel” column for clarity and consistency, as both refers to the same category, (ii) Round values under “Joviality” column to 2 decimal points and (iii) Amend both “participantId” and “householdSize” columns to chr format.\n\n\nCode\npart$educationLevel <- ifelse(part$educationLevel == \"Bachelors\", \"Graduate\", part$educationLevel)\n\npart$joviality <- round(part$joviality, 2)\n\npart$participantId <- as.character(part$participantId)\n\npart$householdSize <- as.character(part$householdSize)\n\n\n\n\n3.3.3 Merging of the 2 Datasets\nBoth dataset (on participants and financial information) are merged into a single base dataset, for the subsequent analysis.\n\n\nCode\n# Merge two data frames using a common column\nmerged_df <- merge(part, fj1_pivoted, by = \"participantId\")\n\nglimpse(merged_df)\n\n\nRows: 10,691\nColumns: 17\n$ participantId     <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ householdSize     <chr> \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3…\n$ haveKids          <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ age               <dbl> 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 25, …\n$ educationLevel    <chr> \"HighSchoolOrCollege\", \"HighSchoolOrCollege\", \"HighS…\n$ interestGroup     <chr> \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H…\n$ joviality         <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ Month_Yr          <chr> \"2022-03\", \"2022-04\", \"2022-05\", \"2022-06\", \"2022-07…\n$ Education         <dbl> 38.01, 38.01, 38.01, 38.01, 38.01, 38.01, 38.01, 38.…\n$ Food              <dbl> 268.34, 265.86, 264.62, 256.97, 270.21, 261.84, 256.…\n$ Recreation        <dbl> 348.72, 219.43, 383.01, 465.68, 1069.54, 314.14, 294…\n$ Shelter           <dbl> 554.99, 554.99, 554.99, 554.99, 554.99, 554.99, 554.…\n$ Wage              <dbl> 11931.95, 8636.88, 9048.16, 9048.16, 8636.88, 9459.4…\n$ RentAdjustment    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Shelter_rev       <dbl> 554.99, 554.99, 554.99, 554.99, 554.99, 554.99, 554.…\n$ Total_income      <dbl> 11931.95, 8636.88, 9048.16, 9048.16, 8636.88, 9459.4…\n$ Total_expenditure <dbl> 1210.06, 1078.29, 1240.63, 1315.65, 1932.75, 1168.98…\n\n\n\n\n\n\n4. Data Visualisation\n\n4.1 Total Expenditure and Total Income Distributions\nTo have an overview on the financial behaviour of the residents, one-sample mean test on the total expenditure for all 1011 representative participants is carried out based on 95% confidence interval.\nThe output shows that the distribution is relatively uniformly distributed. It is observed that the average total expenditure of the population is 18,087 dollars. This almost coincides with the peak value at about 17,500 dollars of nearly 80 counts. It is noted that there were 131 participants with total expenditure less than 1000 dollars. They are treated as outliers and are excluded for the analysis.\n\n\nCode\nmerged_df1 <- merged_df\n\nlibrary(dplyr)\n\n#Group by one column and summarize the other columns by summing up the values within each group\n Income_Exp_summary <- merged_df %>%\n   group_by(`participantId`) %>%\n   summarize(`Food` = sum(`Food`),\n             `Recreation` = sum(`Recreation`),\n             `Shelter_rev` = sum(`Shelter_rev`),\n             `Total_income` = sum(`Total_income`),\n             `Total_expenditure` = sum(`Total_expenditure`))\n\n\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = Income_Exp_summary,\n  x = 'Total_expenditure',\n  type = \"bayes\",\n  test.value = 15000,\n  xlab = \"amount in dollars\" ) +\n  ggtitle(\"Total expenses distribution for all categories\")\n\n\n\n\nCode\nIncome_Exp_summary <- Income_Exp_summary %>%\n  filter(`Total_expenditure` >= 1000)\n\n\nset.seed(1234)\n\ngghistostats(\n  data = Income_Exp_summary,\n  x = 'Total_expenditure',\n  type = \"bayes\",\n  test.value = 15000,\n  xlab = \"amount in dollars\" ) +\n  ggtitle(\"Total expenses distribution for all categories\")\n\n\n\n\n\nWe also investigate the distribution of the total income amount using histogram. Based on the output generated, it is observed that the distribution is heavily right skewed as majority of data is concentrated on the left-hand side of the distribution, with the tail of the distribution extending to the right. The average total expenditure of the population is 31,339 dollars.\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = Income_Exp_summary,\n  x = 'Total_income',\n  type = \"bayes\",\n  test.value = 1500,\n  xlab = \"amount in dollars\" ) +\n  ggtitle(\"Income distribution\")\n\n\n\n\n\n\n\n4.2 Relationship between Total Income and Total Expenditure\nHere, we would like to explore the relationship between total income vs total expenditure of the population. We test the following hypothesis using significant Test of Correlation with ggscatterstats method.\n\nHo there is correlation between total income and total expenditure\nH1: There is no correlation between total income and total expenditure\n\nFrom the output of Student t test, the p value is < 0.05. Thus we reject the hypothesis and conclude that there is no correlation between total income and total expenditure at 95% confidence interval. The Pearson’s correlation coefficient (r), which is a measure of the linear association between two variables, is -0.11 and that also indicates non-correlation between the two variables tested.\n\n\nCode\nggscatterstats(\n  data = Income_Exp_summary,\n  x = Total_expenditure,\n  y = Total_income,\n  marginal = FALSE\n  )\n\n\n\n\n\n\n\n4.3 Trend of Total Expenditure every Month\nFor this analysis, we examine the total expenditure distribution across the twelve months. The mean total expenditure is highest in March 2022 at approximated amount of 1,700 dollars. We can infer this as a bonus payment for the working population possibility due to successful agriculture harvest based on assumption that the City’s population are largely employed in this industry (given that the city serves as a service centre of an agriculture region surrounding it).\n\n\nCode\nmerged_df %>%\n  ggplot(aes(x = Month_Yr, \n             y = Total_expenditure)) +\n  stat_gradientinterval(\n    fill = \"pink\", \n    show.legend = TRUE\n  ) +\n  coord_cartesian(ylim = c(0, 3500)\n  ) + #<<\n  labs(\n    title = \"Visualising confidence intervals of mean total expenditure amount\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\nThe data can be visualised with Hypothetical Outcome Plots (HOPs).\n\n\nCode\nggplot(data = merged_df, \n       (aes(x = factor(Month_Yr), y = Total_expenditure))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = Month_Yr), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n\n4.4 Total Expenditure by Different Age Groups\nWe create multi-ridge plots to determine the total expenditure distribution across different participant age groups to have an idea on how different they could be. The 2 highest age groups register more movements in terms of the ridge shape over the twelve months period while the 21 - 30 age group is relatively stable. It is also observed that the 11-20 age group has more fluctuations from end 2022 onwards.\n\n\nCode\n#binning age values\n\nmerged_df_bin <- merged_df\n\nmerged_df_bin$age <- cut(merged_df_bin$age, breaks = c(0, 10, 20, 30, 40, 50, 60),\n                       labels = c(\"0-10\", \"11-20\", \"21-30\", \"31-40\", \"41-50\", \"51-60\"))\n\nmerged_df_bin$YearMthDay <- as.Date(paste0(merged_df_bin$Month_Yr,\"-01\"))\n\nggplot(data = merged_df_bin, aes(x = Total_expenditure, y = age, fill = after_stat(x))) +\n  \n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n  \n  theme_minimal() +\n  \n    labs(title = 'Total Expenditure by Age: {frame_time}',\n       y = \"Age\",\n       x = \"Total Expenditure amount\") +\n  \n  theme(legend.position=\"none\",\n  text = element_text(family = \"Garamond\"),\n  plot.title = element_text(face = \"bold\", size = 12),\n  \n  axis.title.x = element_text(size = 10, hjust = 1),\n  axis.title.y = element_text(size = 10),\n  axis.text = element_text(size = 8)) +\n  \n  scale_fill_viridis(name = \"Total_expenditure\", option = \"H\") +\n\n  transition_time(merged_df_bin$YearMthDay) +\n  ease_aes('linear')\n\n\n\n\n\nTo obtain deeper insights on the findings, We would like to determine if there are significant differences of total expenditure between age groups, by performing ANOVA test using ggbetweenstats function for non-parametric test. We assume unknown and unequal variance in this case.\n\nHo: the mean total expenditure amount is the same for all ages\nH1: the mean total expenditure amount is different for all ages\n\nBased on the output of the Welch’s test, p > 0.05 and therefore we cannot conclude that there is significant difference exists in the mean total expenditure amount for all age groups.\n\n\nCode\n# Merge the two data frames using a common column\npart_innc_exp_summary <- merge(part, Income_Exp_summary, by = \"participantId\")\n\n#binning age values\npart_innc_exp_summary$age <- cut(part_innc_exp_summary$age, breaks = c(0, 10, 20, 30, 40, 50, 60),\n                       labels = c(\"0-10\", \"11-20\", \"21-30\", \"31-40\", \"41-50\", \"51-60\"))\n\nggbetweenstats(\n  data = part_innc_exp_summary,\n  x = age, \n  y = Total_expenditure,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n4.5 Overall Population Financial Health\nTo determine whether the population is in good financial health, we will develop a metric based on percentage of savings over income to determine its financial health status.\nThe columns are plotted based on various age groups and the highest mean percentage of savings based on 90% confidence interval is found to be 60.7% belonging to the age group from 21 to 30 years old. The lowest mean percentage of savings is 58.7% in the 41 to 50 years old group. We can infer that one of the reasons could be that residents at this age generally have relatively more demanding cost commitments as compared with other age groups. Overall, with the lowest mean percentage saving already more than 50%, we can conclude that the overall financial health of the population is positive.\n\n\nCode\n#create a new data frame\nfin_health <- part_innc_exp_summary\n\n#compute percntage of savings and percentage of expenses as new columns\nfin_health$Pct_savings <- (fin_health$Total_income - fin_health$Total_expenditure) * 100 / fin_health$Total_income\nfin_health$Pct_expenses <- (fin_health$Total_expenditure) * 100 / fin_health$Total_income\n\n\n\n\nCode\ntooltip <- function(y, ymax, accuracy = .01) {\n  mean <- scales::number(y, accuracy = accuracy)\n  sem <- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean Percenage of Total Savings:\", mean, \"+/-\", sem)\n}\n\ngg_point <- ggplot(data=fin_health, \n                   aes(x = age),\n) +\n  stat_summary(aes(y = Pct_savings, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    #fill = \"orange\"\n  ) +\n  stat_summary(aes(y = Pct_savings),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\n4.6 Monthly Percentage of Expenses vs Income\nAn animated bubble plot is created to show the trend of the percentage of total expenses vs total income for the sample participants over the period of twelve months to understand their spending trend using plotly method. It is not unexpected that participants of lower income group have higher percentage of expenses. It is also observed that there were some participants who overspent (percentage of expenses >100%).\n\n\nCode\n # Create new dataframe\n merged_df_fin_health <- merged_df\n\n merged_df_fin_health$Pct_savings <- (merged_df_fin_health$Total_income - merged_df_fin_health$Total_expenditure) * 100 / merged_df_fin_health$Total_income\n\n merged_df_fin_health$Pct_expenses <- (merged_df_fin_health$Total_expenditure) * 100 / merged_df_fin_health$Total_income\n\n # Create new dataframe\n merged_df_fin_health <- merged_df\n\n merged_df_fin_health$Pct_savings <- (merged_df_fin_health$Total_income - merged_df_fin_health$Total_expenditure) * 100 / merged_df_fin_health$Total_income\n\n merged_df_fin_health$Pct_expenses <- (merged_df_fin_health$Total_expenditure) * 100 / merged_df_fin_health$Total_income\n\n\n bp <- merged_df_fin_health %>%\n   plot_ly(x = ~Total_income,\n           y = ~Pct_expenses,\n           size = ~age,\n           color = ~age,\n           sizes = c(2, 30),\n           frame = ~Month_Yr,\n           text = ~participantId,\n           hoverinfo = \"text\",\n           type = 'scatter',\n           mode = 'markers'\n           ) %>%\n   layout(showlegend = FALSE) \n bp\n\n\n\n\n\n\n\n\n4.7 Visualising Financial Health with Other Population Demographics\nUsing the Tree map, we can develop further understanding on the population demography and relative percentage of wages they save by comparing the size of boxes in the plot. Key insights that can be drawn are (i) For residents without kids, the graduates group is the highest in terms of percentage of income saved, and (ii) Residents of low educational qualification has the least percentage of income saved regardless of having kids.\n\n\nCode\nlibrary(treemap)\n\ntreemap_area <- treemap (merged_df_fin_health,\n        index= c(\"haveKids\", \"educationLevel\"),\n        vSize= \"Pct_savings\",\n        vColor = \"Total_income\",\n        type=\"manual\",\n        palette = mako(8),\n        border.col = c(\"black\", \"white\"),\n        title=\"Education Level and Have Kids= TRUE/FALSE\",\n        title.legend = \"Total Income\"\n        )\n\n\n\n\n\n\n\n\n5. Reference\nhttps://www.rdocumentation.org/packages/base/versions/3.6.2/topics/as.Date\nhttps://www.rapidtables.com/math/symbols/Statistical_Symbols.html\nhttps://www.scribbr.com/statistics/t-test/\nhttps://towardsdatascience.com/parametric-tests-the-t-test-c9b17faabfb0\nhttps://stats.stackexchange.com/questions/341553/what-is-bayesian-posterior-probability-and-how-is-it-different-to-just-using-a-p\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/TakeHome2.html",
    "href": "Take-home_Ex/Take-home_Ex02/TakeHome2.html",
    "title": "TakeHome_2",
    "section": "",
    "text": "Overview\n\nIntroduction\nThe country of Oceanus has sought FishEye International’s help in identifying companies possibly engaged in illegal, unreported, and unregulated (IUU) fishing. FishEye’s analysts had received import/export data for Oceanus’ marine and fishing industries, but the data is incomplete. FishEye had subsequently transformed the trade data into a knowledge graph to help them understand the business relationships, including finding links that will stop IUU fishing and protect marine species.\nWe will address Question 1 of VAST challenge 2023 - To Use visual analytics to identify temporal patterns and categorise types of business relationship for individual entities and between entities in FishEye’s knowledge graph.\n\n\nMethodology\nTo select and use different attributes to identify the interactions and relationship among companies and their attributes.\n\n\nData Preparation\n\nInstalling and loading of R packages\nThe code chunk below will be used to install and load the necessary R packages as follows:\n\njsonlite: a package supports both reading and writing JSON files, as well as working with JSON data retrieved from web APIs.\nigraph: a package in R that offers a wide range of tools for creating, manipulating, and visualizing graphs, as well as various algorithms and metrics for network analysis.\ntidygraph: provides a tidy and consistent approach to working with graph data using the principles of the tidyverse\nggraph: an extension package in R that builds upon the tidygraph and ggplot2 packages. It provides a high-level interface for creating visualizations of graph data using the grammar of graphics approach.\nviznetwork: a package provides geoms for ggplot2 to repel overlapping text labels.\nlubridate: a package in R that provides functions for working with dates and times. It aims to simplify common tasks related to date and time manipulation and offers a consistent and intuitive interface.\ntidyverse: a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\n\nCode\npacman::p_load(jsonlite, igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\n\nThe Data\nThe function fromJSON() of jsonlite package is used to import mc2_challenge_graph.json into R environment. The data comprises:\n\n“nodes” dataframe: 34576 observations of 4 variables: shpcountry, rcvcountry, dataset, id\n“links” daraframe with 5464378 observations of 9 variables: arrivaldate, hscodes, valueofgoods_omu, volumetsu, weightkg, dataset, source, target, valueofgoodusd\n\n\n\nCode\nmc2_data <- fromJSON(\"data/mc2_challenge_graph.json\")\n\n\n\nData Preparation\n\nExtracting the edges\nWe extract edges data table from mc2_data list object and save the output in a tibble data frame object called mc2_edges. Together with the nodes, the edges will be used to build and visualise the network graphs for our subsequent analysis. As part of the data wrangling, we will convert the format of data in “ArrivalDate” column to ymd date format, ans use this field to create a new attribute column ” Year”.\n\n\nCode\nmc2_edges <- as_tibble(mc2_data$links) %>%\n  mutate(ArrivalDate = ymd(arrivaldate)) %>%\n  mutate(Year = year(ArrivalDate)) %>%\n  select(source, target, ArrivalDate, Year, hscode, valueofgoods_omu, \n         volumeteu, weightkg, valueofgoodsusd) %>% \n  distinct()\n\n\n\n\nPreparation of edge data\nWe will first filter hscodes that are relevant to seafood for the visual analysis. For example, codes with “03XX” refer to fish, crustaceans and molluscs etc. Codes with “1504” refer to animal fats and oils, those with “1603” to “1605” refers to preparation of fish and molluscus. “2301” ,which refers to meals and pellets of fish/crustaceans/molluscs, is also used. In addition, it is observed that many columns of the dataset have “NA” values, and they will not be used further. We have also focus on those nodes with higher number of edges connections to target our analysis.\n\n\nCode\n#filter the relevant hscodes for analysis\nhscodes_filtered <- c(\"0301\", \"0302\", \"0303\", \"0304\", \"0305\",\"0306\", \"0307\", \"0308\", \"0309\", \"1504\",\"1603\", \"1604\", \"1605\", \"2301\")\n\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(substr(as.character(hscode), 1, 4) %in% hscodes_filtered) %>%\n  #filter(hscode %in% hscodes_filtered) %>%\n  group_by(source, target, hscode, Year) %>%\n    summarise(weights = n()) %>%\n  filter(source!=target) %>%\n  #filter(weights > 50) %>%\n  ungroup()\n\n\n\n\nPreparing nodes data\nInstead of using the nodes data table extracted from mc2_data, we will prepare a new nodes data table by using the source and target fields of mc2_edges_aggregated data table. This is necessary to ensure that the nodes in nodes data tables include all the source and target values.\n\n\nCode\nid1 <- mc2_edges_aggregated %>%\n  select(source) %>%\n  rename(id = source)\nid2 <- mc2_edges_aggregated %>%\n  select(target) %>%\n  rename(id = target)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n\nWe use the following code chunk to create a tbl_graph object.\n\n\nCode\nmc2_graph <- tbl_graph(nodes = mc2_nodes_extracted,\n                       edges = mc2_edges_aggregated,\n                       directed = TRUE)\n\n\n\n\nComputation of Centrality Metrices\nNext, we compute the necessary centrality metrics and stored in respective columns created in the graph object. The respective metric values will be extracted accordingly and be used in the subsequent network plots in the visual analysis.\n\n\nCode\ngraph1 <- mc2_graph %>%\n\n  activate(nodes) %>%\n  #as_tibble() %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n    mutate(deg_bin0 = cut(betweenness_centrality, breaks = c(0, 3000, 7000, Inf),\n                       labels = c(\"Low\\n(0-2999)\", \n                                  \"Medium\\n(3000-6999)\", \n                                  \"High\\n(>=7000)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  \n  mutate(in_degree_centrality = degree(mc2_graph, mode = \"in\")) %>%\n   mutate(deg_bin1 = cut(in_degree_centrality, breaks = c(0, 200, 300, Inf),\n                       labels = c(\"Low\\n(0-199)\", \n                                  \"Medium\\n(200-299)\", \n                                  \"High\\n(>=300)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  mutate(out_degree_centrality = degree(mc2_graph, mode = \"out\")) %>%\n    mutate(deg_bin2 = cut(out_degree_centrality, breaks = c(0, 175, 300, Inf),\n                       labels = c(\"Low\\n(0-174)\", \n                                  \"Medium\\n(175-299)\", \n                                  \"High\\n(>=300)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  mutate(closeness_centrality = round(centrality_closeness(),2)) %>%\n     mutate(deg_bin3 = cut(closeness_centrality, breaks = c(0, 0.3, 0.6, Inf),\n                       labels = c(\"Low\\n(0-0.29)\", \n                                  \"Medium\\n(0.3-0.59)\", \n                                  \"High\\n(>=0.6)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  mutate(Eigenvalue_centrality = round(centrality_eigen(),2)) %>%\n      mutate(deg_bin4 = cut(Eigenvalue_centrality, breaks = c(0, 0.3, 0.6, Inf),\n                       labels = c(\"Low\\n(0-0.29)\", \n                                  \"Medium\\n(0.3-0.59)\", \n                                  \"High\\n(>=0.6)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  mutate(clustering_coefficient = round(transitivity(mc2_graph, type = \"local\"),2)) %>%\n   mutate(deg_bin5 = cut(clustering_coefficient, breaks = c(0, 0.3, 0.6, Inf),\n                       labels = c(\"Low\\n(0-0.29)\", \n                                  \"Medium\\n(0.3-0.59)\", \n                                  \"High\\n(>=0.6)\\n\"),  \n                       include.lowest = TRUE))\n\n\n\n\nPreparing the base nodes and edges for visualisation\nTo create the interactive plots, we will need to prepare the data. This is first done by creating the master edges and master modes from the graph object earlier. The master edges and master nodes will be manipulated to develop the different visual presentations on the respective centrality metrices.\n\n\nCode\n# create master edges\nmain_edges <- mc2_edges_aggregated\n\n\n\n\nCode\n#create master nodes\nmain_nodes <- graph1 %>%\n  activate(\"nodes\") %>%\n  as_tibble()\n\n\n\n\n\n\n\n\n\nVisualisation\n\nVisualisation and Analysis of Betweenness Centrality\nBetweenness centrality measures the extent to which a node lies on the shortest paths between other pairs of nodes in the network. It also quantifies the node’s influence as a bridge or intermediary in the flow of information or resources within the network.By identifying nodes with high betweenness centrality, network analysts can gain insights into the nodes that are crucial for maintaining efficient communication, facilitating the spread of information, and controlling the network’s overall connectivity.\nThe colour code from the plot allows us to identify the nodes with high, medium and low betweenness centrality values. For instance, Adriatic Tuna Seabass BV Transit and Selous Game Reserve S.A. de C.V are identified as nodes with high betweenness with multiple incoming links as well as out going links to other nodes (e.g. such as to hai dan Corporation Wharf etc.). We can infer that these could be intermediates that transport the catch from the fishing boats/companies to the wharfs/distribution houses.\n\n\nCode\n#filter and reduce nodes\nnodes_between <- main_nodes %>%\n  arrange(desc(betweenness_centrality)) %>%\n  slice_max(order_by = betweenness_centrality, n = 5) \n\n\n\n\nCode\n# filter edges based on reduced nodes\nedges_between  <- main_edges %>%\n  filter(source %in% nodes_between$id | target %in% nodes_between$id) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_between_plot  <- main_nodes %>%\n  filter(id %in% c(edges_between$source, edges_between$target)) %>%\n  rename(group = deg_bin0)  %>%\n  arrange(id)\n\n\n\n\nCode\n#prep format for plotting\nedges_between <- edges_between %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_between_plot, edges_between) %>%\n  visEdges(arrows = \"to\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123) \n\n\n\n\n\n\n\n\n\n\n\nVisualisation and Analysis of In_degree Centrality\nThe in_degree centrality reflects the number of other nodes that have edges pointing towards the node, indicating the node’s popularity or importance in terms of receiving connections or information from other nodes. Nodes with high in-degree centrality are often seen as influential or important in terms of receiving information, resources, or influence from other nodes.\nIn this network, we can see that entities such as Mar del Este CJSC and hai dan Corporation Wharf have high numbers of incoming edges from other nodes. These entities could be wharfs/distribution houses where the catch is delivered to them.\n\n\nCode\n#filter and reduce nodes\nnodes_indegree <- main_nodes %>%\n  arrange(desc(in_degree_centrality)) %>%\n  #filter(in_degree_centrality > 1000)\n  slice_max(order_by = in_degree_centrality, n = 10)\n\n\n\n\nCode\n# filter edges based on reduced nodes\nedges_indegree  <- main_edges %>%\n  filter(source %in% nodes_indegree$id | target %in% nodes_indegree$id) %>%\n  filter(weights > 50) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_indegree_plot  <- main_nodes %>%\n  filter(id %in% c(edges_indegree$source, edges_indegree$target)) %>%\n  rename(group = deg_bin1)  %>%\n  arrange(id)\n\n\n\n\nCode\n#prep format for plotting\nedges_indegree <- edges_indegree %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_indegree_plot, edges_indegree) %>%\n  visEdges(arrows = \"to\") %>%\n  visIgraphLayout(layout = \"layout_with_kk\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nVisualisation and Analysis of Out_degree Centrality\nOut-degree centrality specifically measures the number of outgoing edges from a node in a directed graph, indicating the node’s ability to send connections or information to other nodes. It quantifies the number of other nodes that the node is connected to with outgoing edges.\nWE can infer from the plot that Shou gan Oyj Overseas and Oceano del Este SRL are two nodes with the highest out_degree centrality value and they could be large fishing companies. It is also oberved that their outgoing links goes to logistics and transport companies.\n\n\nCode\n#filter and reduce nodes\nnodes_outdegree <- main_nodes %>%\n  arrange(desc(out_degree_centrality)) %>%\n  slice_max(order_by = out_degree_centrality, n = 5)\n\n\n\n\nCode\n# filter edges based on reduced nodes\nnodes_outdegree <- nodes_outdegree\nedges_outdegree  <- main_edges %>%\n  filter(source %in% nodes_outdegree$id | target %in% nodes_outdegree$id) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_outdegree_plot  <- main_nodes %>%\n  filter(id %in% c(edges_outdegree$source, edges_outdegree$target)) %>%\n  rename(group = deg_bin2)  %>%\n  arrange(id)\n\n#prep format for plotting\nedges_outdegree <- edges_outdegree %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_outdegree_plot, edges_outdegree) %>%\n  visEdges(arrows = \"to\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nCloseness Centrality\nCloseness centrality is a measure used in network analysis to quantify the centrality of nodes based on their proximity to other nodes in the network. It measures how close a node is to all other nodes in terms of the shortest path distances.The closer a node is to all other nodes, the higher its closeness centrality. One example of nodes that are high in closeness centrality is Estrella del Mar Tilapia Oyj Marine which is linked to Adriatic Tuna Seabass BV Transit which has the hightest betweeness centrality score.\n\n\nCode\n#compute centrality and filter high value nodes\nnodes_closeness <- main_nodes %>%\n  arrange(desc(closeness_centrality)) %>%\n  #filter(betweenness_centrality > 1000000)\n  slice_max(order_by = closeness_centrality, n = 900)\n\n\n\n\nCode\n# filter edges based on filtered nodes\nedges_closeness <- main_edges %>%\n  filter(source %in% nodes_closeness$id | target %in% nodes_closeness$id) %>%\n  filter(weights > 50) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_closeness_plot  <- main_nodes %>%\n  filter(id %in% c(edges_closeness$source, edges_closeness$target)) %>%\n  rename(group = deg_bin3)  %>%\n  arrange(id)\n\n\n#prep format\nedges_closeness <- edges_closeness %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_closeness_plot, edges_closeness) %>%\n  visEdges(arrows = \"to\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nEigenvalue Centrality\nEigenvalue centrality is a measure of node centrality in a network based on the concept of eigenvectors.The idea behind eigenvalue centrality is that a node is considered central if it is connected to other nodes that are themselves central. Thus, the centrality of a node depends not only on the number of connections it has but also on the centrality of those connections. Not surprisingly Mar Del Este CJS and hai dan Corporation Wharf have the highest eigenvalue scores here as they are wharfs/distribution houses and have high upstream connectivity with other “central” entities such as intermediaries.\n\n\nCode\n#filter and reduce nodes\nnodes_eigen <- main_nodes %>%\n  arrange(desc(Eigenvalue_centrality)) %>%\n  #filter(betweenness_centrality > 1000000)\n  slice_max(order_by = Eigenvalue_centrality, n = 15)\n\n\n\n\nCode\n# filter edges based on reduced nodes\nedges_eigen  <- main_edges %>%\n  filter(source %in% nodes_eigen$id | target %in% nodes_eigen$id) %>%\n  filter(weights > 70) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_eigen_plot  <- main_nodes %>%\n  filter(id %in% c(edges_eigen$source, edges_eigen$target)) %>%\n  rename(group = deg_bin4)  %>%\n  arrange(id)\n\n\n#prep format for plotting\nedges_eigen <- edges_eigen %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_eigen_plot, edges_eigen) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of In_degree centrality across the years\nThe In_degree network graphs from 2028 to 2034 are plotted for comparison in a facet. This will provide us with an overview on how the trend of In_degree centrality metric changes over time. We will also closely examine the difference between the first and last year for more insights.\n\n\nCode\n#set as graph object using nodes and edges for in_degree centrality\nstaticgraph_indegree <- tbl_graph(nodes = nodes_indegree_plot,\n                       edges = edges_indegree,\n                       directed = TRUE)\n\n\n#Plotting the graph\nset_graph_style() \n\ng <- ggraph(staticgraph_indegree, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = group), \n                  size = 1)\n\n#putting as a facet presentation  \ng + facet_edges(~Year) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn_degree Centrality analysis in Year 2028\nFrom the plot, it is observed that hai dan Corporation Wharf has received the most incoming links from Goa Seaside So Overseas as well as SeaSelect Foods Salt spray, as shown by the weights (thickness) of the edges between these nodes. For Mar Del Este CJS and Pao gan SE Seal, their biggest supplier are Danish Place Swordfish AB Shipping and Saltsea & Inc Carriers respectively.\n\n\nCode\n# filter edges based on reduced nodes\nedges_indegree2028  <- main_edges %>%\n  filter(source %in% nodes_indegree$id | target %in% nodes_indegree$id) %>%\n  filter(Year == 2028) %>%\n  filter(weights > 50) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_indegree_plot2028  <- main_nodes %>%\n  filter(id %in% c(edges_indegree2028$source, edges_indegree2028$target)) %>%\n  rename(group = deg_bin1)  %>%\n  arrange(id)\n\n\n#set as graph object using nodes and edges for in_degree centrality\nstaticgraph_indegree <- tbl_graph(nodes = nodes_indegree_plot2028,\n                       edges = edges_indegree2028,\n                       directed = TRUE)\n\n\n#Plotting the graph\nset_graph_style() \n\ng <- ggraph(staticgraph_indegree, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = group), \n                  size = 2) \n\n#putting as a facet presentation  \ng +\n  geom_node_text(aes(label = id), size = 2, repel=TRUE) +\n  ggtitle(\"In_degree Centrality Plot in Year 2028\") + \n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn_degree Centrality analysis in Year 2034\nMoving forward to 2034, we can observe that the landscape has totally changed from 2028. LLC SA de CV, who used to supply to hai dan Corporation Wharf, is now their biggest supplier in 2034. Similarly, Mar Del Este CJS’s biggest supplier is now Estrella de la Costa who had also supplied them six years ago, abeit at a reduced weight (edges). Interestingly, we do not see Pao gan SE Seal, one of the nodes that is high in in_degree centrality score. By such comparisons between each year, we can chart the movement of the entities in FishEye’s knowledge graph and obtain deeper insights on their characteristics and behaviours.\n\n\nCode\n# filter edges based on reduced nodes\nedges_indegree2034  <- main_edges %>%\n  filter(source %in% nodes_indegree$id | target %in% nodes_indegree$id) %>%\n  filter(Year == 2034) %>%\n  filter(weights > 80) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_indegree_plot2034  <- main_nodes %>%\n  filter(id %in% c(edges_indegree2034$source, edges_indegree2034$target)) %>%\n  rename(group = deg_bin1)  %>%\n  arrange(id)\n\n\n#set as graph object using nodes and edges for in_degree centrality\nstaticgraph_indegree <- tbl_graph(nodes = nodes_indegree_plot2034,\n                       edges = edges_indegree2034,\n                       directed = TRUE)\n\n\n#Plotting the graph\nset_graph_style() \n\ng <- ggraph(staticgraph_indegree, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = group), \n                  size = 2) \n\n#putting as a facet presentation  \ng +\n  geom_node_text(aes(label = id), size = 2, repel=TRUE) +\n  ggtitle(\"In_degree Centrality Plot in Year 2034\") + \n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\nConclusion\nThe visualisation and analysis of network data has helps us in understanding complex relationships and uncovering patterns within networks. It has enabled us to uncover valuable insights on central entities and influential nodes, and gain a deeper understanding of the complex network of business relationships of the entities within FishEye’s knowledge graph."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/TKEx02.html",
    "href": "Take-home_Ex/Take-home_Ex02/TKEx02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Overview\n\nIntroduction\nThe country of Oceanus has sought FishEye International’s help in identifying companies possibly engaged in illegal, unreported, and unregulated (IUU) fishing. FishEye’s analysts had received import/export data for Oceanus’ marine and fishing industries, but the data is incomplete. FishEye had subsequently transformed the trade data into a knowledge graph to help them understand the business relationships, including finding links that will stop IUU fishing and protect marine species.\nWe will address Question 1 of VAST challenge 2023 - To Use visual analytics to identify temporal patterns and categorise types of business relationship for individual entities and between entities in FishEye’s knowledge graph.\n\n\nMethodology\nTo select and use different attributes to identify the interactions and relationship among companies and their attributes.\n\n\nData Preparation\n\nInstalling and loading of R packages\nThe code chunk below will be used to install and load the necessary R packages as follows:\n\njsonlite: a package supports both reading and writing JSON files, as well as working with JSON data retrieved from web APIs.\nigraph: a package in R that offers a wide range of tools for creating, manipulating, and visualizing graphs, as well as various algorithms and metrics for network analysis.\ntidygraph: provides a tidy and consistent approach to working with graph data using the principles of the tidyverse\nggraph: an extension package in R that builds upon the tidygraph and ggplot2 packages. It provides a high-level interface for creating visualizations of graph data using the grammar of graphics approach.\nviznetwork: a package provides geoms for ggplot2 to repel overlapping text labels.\nlubridate: a package in R that provides functions for working with dates and times. It aims to simplify common tasks related to date and time manipulation and offers a consistent and intuitive interface.\ntidyverse: a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\n\nCode\npacman::p_load(jsonlite, igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\n\nThe Data\nThe function fromJSON() of jsonlite package is used to import mc2_challenge_graph.json into R environment. The data comprises:\n\n“nodes” dataframe: 34576 observations of 4 variables: shpcountry, rcvcountry, dataset, id\n“links” daraframe with 5464378 observations of 9 variables: arrivaldate, hscodes, valueofgoods_omu, volumetsu, weightkg, dataset, source, target, valueofgoodusd\n\n\n\nCode\nmc2_data <- fromJSON(\"data/mc2_challenge_graph.json\")\n\n\n\nData Preparation\n\nExtracting the edges\nWe extract edges data table from mc2_data list object and save the output in a tibble data frame object called mc2_edges. Together with the nodes, the edges will be used to build and visualise the network graphs for our subsequent analysis. As part of the data wrangling, we will convert the format of data in “ArrivalDate” column to ymd date format, ans use this field to create a new attribute column ” Year”.\n\n\nCode\nmc2_edges <- as_tibble(mc2_data$links) %>%\n  mutate(ArrivalDate = ymd(arrivaldate)) %>%\n  mutate(Year = year(ArrivalDate)) %>%\n  select(source, target, ArrivalDate, Year, hscode, valueofgoods_omu, \n         volumeteu, weightkg, valueofgoodsusd) %>% \n  distinct()\n\n\n\n\nPreparation of edge data\nWe will first filter hscodes that are relevant to seafood for the visual analysis. For example, codes with “03XX” refer to fish, crustaceans and molluscs etc. Codes with “1504” refer to animal fats and oils, those with “1603” to “1605” refers to preparation of fish and molluscus. “2301” ,which refers to meals and pellets of fish/crustaceans/molluscs, is also used. In addition, it is observed that many columns of the dataset have “NA” values, and they will not be used further. We have also focus on those nodes with higher number of edges connections to target our analysis.\n\n\nCode\n#filter the relevant hscodes for analysis\nhscodes_filtered <- c(\"0301\", \"0302\", \"0303\", \"0304\", \"0305\",\"0306\", \"0307\", \"0308\", \"0309\", \"1504\",\"1603\", \"1604\", \"1605\", \"2301\")\n\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(substr(as.character(hscode), 1, 4) %in% hscodes_filtered) %>%\n  #filter(hscode %in% hscodes_filtered) %>%\n  group_by(source, target, hscode, Year) %>%\n    summarise(weights = n()) %>%\n  filter(source!=target) %>%\n  #filter(weights > 50) %>%\n  ungroup()\n\n\n\n\nPreparing nodes data\nInstead of using the nodes data table extracted from mc2_data, we will prepare a new nodes data table by using the source and target fields of mc2_edges_aggregated data table. This is necessary to ensure that the nodes in nodes data tables include all the source and target values.\n\n\nCode\nid1 <- mc2_edges_aggregated %>%\n  select(source) %>%\n  rename(id = source)\nid2 <- mc2_edges_aggregated %>%\n  select(target) %>%\n  rename(id = target)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n\nWe use the following code chunk to create a tbl_graph object.\n\n\nCode\nmc2_graph <- tbl_graph(nodes = mc2_nodes_extracted,\n                       edges = mc2_edges_aggregated,\n                       directed = TRUE)\n\n\n\n\nComputation of Centrality Metrices\nNext, we compute the necessary centrality metrics and stored in respective columns created in the graph object. The respective metric values will be extracted accordingly and be used in the subsequent network plots in the visual analysis.\n\n\nCode\ngraph1 <- mc2_graph %>%\n\n  activate(nodes) %>%\n  #as_tibble() %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n    mutate(deg_bin0 = cut(betweenness_centrality, breaks = c(0, 3000, 7000, Inf),\n                       labels = c(\"Low\\n(0-2999)\", \n                                  \"Medium\\n(3000-6999)\", \n                                  \"High\\n(>=7000)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  \n  mutate(in_degree_centrality = degree(mc2_graph, mode = \"in\")) %>%\n   mutate(deg_bin1 = cut(in_degree_centrality, breaks = c(0, 200, 300, Inf),\n                       labels = c(\"Low\\n(0-199)\", \n                                  \"Medium\\n(200-299)\", \n                                  \"High\\n(>=300)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  mutate(out_degree_centrality = degree(mc2_graph, mode = \"out\")) %>%\n    mutate(deg_bin2 = cut(out_degree_centrality, breaks = c(0, 175, 300, Inf),\n                       labels = c(\"Low\\n(0-174)\", \n                                  \"Medium\\n(175-299)\", \n                                  \"High\\n(>=300)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  mutate(closeness_centrality = round(centrality_closeness(),2)) %>%\n     mutate(deg_bin3 = cut(closeness_centrality, breaks = c(0, 0.3, 0.6, Inf),\n                       labels = c(\"Low\\n(0-0.29)\", \n                                  \"Medium\\n(0.3-0.59)\", \n                                  \"High\\n(>=0.6)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  mutate(Eigenvalue_centrality = round(centrality_eigen(),2)) %>%\n      mutate(deg_bin4 = cut(Eigenvalue_centrality, breaks = c(0, 0.3, 0.6, Inf),\n                       labels = c(\"Low\\n(0-0.29)\", \n                                  \"Medium\\n(0.3-0.59)\", \n                                  \"High\\n(>=0.6)\\n\"),  \n                       include.lowest = TRUE))  %>%\n  \n  mutate(clustering_coefficient = round(transitivity(mc2_graph, type = \"local\"),2)) %>%\n   mutate(deg_bin5 = cut(clustering_coefficient, breaks = c(0, 0.3, 0.6, Inf),\n                       labels = c(\"Low\\n(0-0.29)\", \n                                  \"Medium\\n(0.3-0.59)\", \n                                  \"High\\n(>=0.6)\\n\"),  \n                       include.lowest = TRUE))\n\n\n\n\nPreparing the base nodes and edges for visualisation\nTo create the interactive plots, we will need to prepare the data. This is first done by creating the master edges and master modes from the graph object earlier. The master edges and master nodes will be manipulated to develop the different visual presentations on the respective centrality metrices.\n\n\nCode\n# create master edges\nmain_edges <- mc2_edges_aggregated\n\n\n\n\nCode\n#create master nodes\nmain_nodes <- graph1 %>%\n  activate(\"nodes\") %>%\n  as_tibble()\n\n\n\n\n\n\n\n\n\nVisualisation\n\nVisualisation and Analysis of Betweenness Centrality\nBetweenness centrality measures the extent to which a node lies on the shortest paths between other pairs of nodes in the network. It also quantifies the node’s influence as a bridge or intermediary in the flow of information or resources within the network.By identifying nodes with high betweenness centrality, network analysts can gain insights into the nodes that are crucial for maintaining efficient communication, facilitating the spread of information, and controlling the network’s overall connectivity.\nThe colour code from the plot allows us to identify the nodes with high, medium and low betweenness centrality values. For instance, Adriatic Tuna Seabass BV Transit and Selous Game Reserve S.A. de C.V are identified as nodes with high betweenness with multiple incoming links as well as out going links to other nodes (e.g. such as to hai dan Corporation Wharf etc.). We can infer that these could be intermediates that transport the catch from the fishing boats/companies to the wharfs/distribution houses.\n\n\nCode\n#filter and reduce nodes\nnodes_between <- main_nodes %>%\n  arrange(desc(betweenness_centrality)) %>%\n  slice_max(order_by = betweenness_centrality, n = 5) \n\n\n\n\nCode\n# filter edges based on reduced nodes\nedges_between  <- main_edges %>%\n  filter(source %in% nodes_between$id | target %in% nodes_between$id) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_between_plot  <- main_nodes %>%\n  filter(id %in% c(edges_between$source, edges_between$target)) %>%\n  rename(group = deg_bin0)  %>%\n  arrange(id)\n\n\n\n\nCode\n#prep format for plotting\nedges_between <- edges_between %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_between_plot, edges_between) %>%\n  visEdges(arrows = \"to\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123) \n\n\n\n\n\n\n\n\n\n\n\nVisualisation and Analysis of In_degree Centrality\nThe in_degree centrality reflects the number of other nodes that have edges pointing towards the node, indicating the node’s popularity or importance in terms of receiving connections or information from other nodes. Nodes with high in-degree centrality are often seen as influential or important in terms of receiving information, resources, or influence from other nodes.\nIn this network, we can see that entities such as Mar del Este CJSC and hai dan Corporation Wharf have high numbers of incoming edges from other nodes. These entities could be wharfs/distribution houses where the catch is delivered to them.\n\n\nCode\n#filter and reduce nodes\nnodes_indegree <- main_nodes %>%\n  arrange(desc(in_degree_centrality)) %>%\n  #filter(in_degree_centrality > 1000)\n  slice_max(order_by = in_degree_centrality, n = 10)\n\n\n\n\nCode\n# filter edges based on reduced nodes\nedges_indegree  <- main_edges %>%\n  filter(source %in% nodes_indegree$id | target %in% nodes_indegree$id) %>%\n  filter(weights > 50) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_indegree_plot  <- main_nodes %>%\n  filter(id %in% c(edges_indegree$source, edges_indegree$target)) %>%\n  rename(group = deg_bin1)  %>%\n  arrange(id)\n\n\n\n\nCode\n#prep format for plotting\nedges_indegree <- edges_indegree %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_indegree_plot, edges_indegree) %>%\n  visEdges(arrows = \"to\") %>%\n  visIgraphLayout(layout = \"layout_with_kk\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nVisualisation and Analysis of Out_degree Centrality\nOut-degree centrality specifically measures the number of outgoing edges from a node in a directed graph, indicating the node’s ability to send connections or information to other nodes. It quantifies the number of other nodes that the node is connected to with outgoing edges.\nWE can infer from the plot that Shou gan Oyj Overseas and Oceano del Este SRL are two nodes with the highest out_degree centrality value and they could be large fishing companies. It is also oberved that their outgoing links goes to logistics and transport companies.\n\n\nCode\n#filter and reduce nodes\nnodes_outdegree <- main_nodes %>%\n  arrange(desc(out_degree_centrality)) %>%\n  slice_max(order_by = out_degree_centrality, n = 5)\n\n\n\n\nCode\n# filter edges based on reduced nodes\nnodes_outdegree <- nodes_outdegree\nedges_outdegree  <- main_edges %>%\n  filter(source %in% nodes_outdegree$id | target %in% nodes_outdegree$id) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_outdegree_plot  <- main_nodes %>%\n  filter(id %in% c(edges_outdegree$source, edges_outdegree$target)) %>%\n  rename(group = deg_bin2)  %>%\n  arrange(id)\n\n#prep format for plotting\nedges_outdegree <- edges_outdegree %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_outdegree_plot, edges_outdegree) %>%\n  visEdges(arrows = \"to\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nCloseness Centrality\nCloseness centrality is a measure used in network analysis to quantify the centrality of nodes based on their proximity to other nodes in the network. It measures how close a node is to all other nodes in terms of the shortest path distances.The closer a node is to all other nodes, the higher its closeness centrality. One example of nodes that are high in closeness centrality is Estrella del Mar Tilapia Oyj Marine which is linked to Adriatic Tuna Seabass BV Transit which has the hightest betweeness centrality score.\n\n\nCode\n#compute centrality and filter high value nodes\nnodes_closeness <- main_nodes %>%\n  arrange(desc(closeness_centrality)) %>%\n  #filter(betweenness_centrality > 1000000)\n  slice_max(order_by = closeness_centrality, n = 900)\n\n\n\n\nCode\n# filter edges based on filtered nodes\nedges_closeness <- main_edges %>%\n  filter(source %in% nodes_closeness$id | target %in% nodes_closeness$id) %>%\n  filter(weights > 50) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_closeness_plot  <- main_nodes %>%\n  filter(id %in% c(edges_closeness$source, edges_closeness$target)) %>%\n  rename(group = deg_bin3)  %>%\n  arrange(id)\n\n\n#prep format\nedges_closeness <- edges_closeness %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_closeness_plot, edges_closeness) %>%\n  visEdges(arrows = \"to\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nEigenvalue Centrality\nEigenvalue centrality is a measure of node centrality in a network based on the concept of eigenvectors.The idea behind eigenvalue centrality is that a node is considered central if it is connected to other nodes that are themselves central. Thus, the centrality of a node depends not only on the number of connections it has but also on the centrality of those connections. Not surprisingly Mar Del Este CJS and hai dan Corporation Wharf have the highest eigenvalue scores here as they are wharfs/distribution houses and have high upstream connectivity with other “central” entities such as intermediaries.\n\n\nCode\n#filter and reduce nodes\nnodes_eigen <- main_nodes %>%\n  arrange(desc(Eigenvalue_centrality)) %>%\n  #filter(betweenness_centrality > 1000000)\n  slice_max(order_by = Eigenvalue_centrality, n = 15)\n\n\n\n\nCode\n# filter edges based on reduced nodes\nedges_eigen  <- main_edges %>%\n  filter(source %in% nodes_eigen$id | target %in% nodes_eigen$id) %>%\n  filter(weights > 70) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_eigen_plot  <- main_nodes %>%\n  filter(id %in% c(edges_eigen$source, edges_eigen$target)) %>%\n  rename(group = deg_bin4)  %>%\n  arrange(id)\n\n\n#prep format for plotting\nedges_eigen <- edges_eigen %>%\nrename(from = source) %>%\nrename(to = target) %>%\n#filter (Year == \"2028\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n# Plot network graph\nvisNetwork(nodes_eigen_plot, edges_eigen) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of In_degree centrality across the years\nThe In_degree network graphs from 2028 to 2034 are plotted for comparison in a facet. This will provide us with an overview on how the trend of In_degree centrality metric changes over time. We will also closely examine the difference between the first and last year for more insights.\n\n\nCode\n#set as graph object using nodes and edges for in_degree centrality\nstaticgraph_indegree <- tbl_graph(nodes = nodes_indegree_plot,\n                       edges = edges_indegree,\n                       directed = TRUE)\n\n\n#Plotting the graph\nset_graph_style() \n\ng <- ggraph(staticgraph_indegree, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = group), \n                  size = 1)\n\n#putting as a facet presentation  \ng + facet_edges(~Year) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn_degree Centrality analysis in Year 2028\nFrom the plot, it is observed that hai dan Corporation Wharf has received the most incoming links from Goa Seaside So Overseas as well as SeaSelect Foods Salt spray, as shown by the weights (thickness) of the edges between these nodes. For Mar Del Este CJS and Pao gan SE Seal, their biggest supplier are Danish Place Swordfish AB Shipping and Saltsea & Inc Carriers respectively.\n\n\nCode\n# filter edges based on reduced nodes\nedges_indegree2028  <- main_edges %>%\n  filter(source %in% nodes_indegree$id | target %in% nodes_indegree$id) %>%\n  filter(Year == 2028) %>%\n  filter(weights > 50) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_indegree_plot2028  <- main_nodes %>%\n  filter(id %in% c(edges_indegree2028$source, edges_indegree2028$target)) %>%\n  rename(group = deg_bin1)  %>%\n  arrange(id)\n\n\n#set as graph object using nodes and edges for in_degree centrality\nstaticgraph_indegree <- tbl_graph(nodes = nodes_indegree_plot2028,\n                       edges = edges_indegree2028,\n                       directed = TRUE)\n\n\n#Plotting the graph\nset_graph_style() \n\ng <- ggraph(staticgraph_indegree, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = group), \n                  size = 2) \n\n#putting as a facet presentation  \ng +\n  geom_node_text(aes(label = id), size = 2, repel=TRUE) +\n  ggtitle(\"In_degree Centrality Plot in Year 2028\") + \n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn_degree Centrality analysis in Year 2034\nMoving forward to 2034, we can observe that the landscape has totally changed from 2028. LLC SA de CV, who used to supply to hai dan Corporation Wharf, is now their biggest supplier in 2034. Similarly, Mar Del Este CJS’s biggest supplier is now Estrella de la Costa who had also supplied them six years ago, abeit at a reduced weight (edges). Interestingly, we do not see Pao gan SE Seal, one of the nodes that is high in in_degree centrality score. By such comparisons between each year, we can chart the movement of the entities in FishEye’s knowledge graph and obtain deeper insights on their characteristics and behaviours.\n\n\nCode\n# filter edges based on reduced nodes\nedges_indegree2034  <- main_edges %>%\n  filter(source %in% nodes_indegree$id | target %in% nodes_indegree$id) %>%\n  filter(Year == 2034) %>%\n  filter(weights > 80) %>%\n  arrange(source, target)\n\n#filter nodes based on filtered edge nodes for plotting\nnodes_indegree_plot2034  <- main_nodes %>%\n  filter(id %in% c(edges_indegree2034$source, edges_indegree2034$target)) %>%\n  rename(group = deg_bin1)  %>%\n  arrange(id)\n\n\n#set as graph object using nodes and edges for in_degree centrality\nstaticgraph_indegree <- tbl_graph(nodes = nodes_indegree_plot2034,\n                       edges = edges_indegree2034,\n                       directed = TRUE)\n\n\n#Plotting the graph\nset_graph_style() \n\ng <- ggraph(staticgraph_indegree, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = group), \n                  size = 2) \n\n#putting as a facet presentation  \ng +\n  geom_node_text(aes(label = id), size = 2, repel=TRUE) +\n  ggtitle(\"In_degree Centrality Plot in Year 2034\") + \n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\nConclusion\nThe visualisation and analysis of network data has helps us in understanding complex relationships and uncovering patterns within networks. It has enabled us to uncover valuable insights on central entities and influential nodes, and gain a deeper understanding of the complex network of business relationships of the entities within FishEye’s knowledge graph."
  },
  {
    "objectID": "VAProject/VAprojectmc3.html",
    "href": "VAProject/VAprojectmc3.html",
    "title": "Project",
    "section": "",
    "text": "Code\npacman::p_load(jsonlite, igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\nCode\nread_mc3_nodes <- read_rds(\"data/mc3_nodes.rds\")\n\n\n\n\nCode\nread_mc3_edges <- read_rds(\"data/mc3_edges.rds\")\n\n\n#read_mc3_nodes <- read_rds(“data/generated/mc3_nodes.rds”) #read_mc3_edges <- read_rds(“data/generated/mc3_edges.rds”)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "21 Choropleth Mapping with R\n\n\n21.1 Overview\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package.\n\n\n21.2 Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n21.3 Importing Data into R\n\n21.3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n21.3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SMU_R\\chanhpsmu\\ISSS608_VAA\\Hands-on_Ex\\Hands-on_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n21.3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\n\nCode\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n21.3.4 Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\n21.3.4.1 Data Wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n21.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\n\n{r} # write_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\") #\n\n\n21.4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n21.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases. The code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n21.4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n21.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n21.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n21.4.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n21.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n21.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n21.4.3.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n21.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n21.4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n21.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n21.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n21.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n21.4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code chunk below\n\ntmap_style(\"white\")\n\n\n\n\n21.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n21.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n21.4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\ntm_fill(\"DEPENDENCY\",\nstyle = \"quantile\",\npalette = \"Blues\",\nthres.poly = 0) +\ntm_facets(by=\"REGION_N\",\nfree.coords=TRUE,\ndrop.shapes=TRUE) +\ntm_layout(legend.show = FALSE,\ntitle.position = c(\"center\", \"center\"),\ntitle.size = 20) +\ntm_borders(alpha = 0.5)\n\n\n\n#### 21.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n21.4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex08/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608_VAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-Home Exercise 3",
    "section": "",
    "text": "Overview\n\nIntroduction\nThe country of Oceanus has sought FishEye International’s help in identifying companies possibly engaged in illegal, unreported, and unregulated (IUU) fishing. FishEye’s analysts had received import/export data for Oceanus’ marine and fishing industries, but the data is incomplete. FishEye had subsequently transformed the trade data into a knowledge graph to help them understand the business relationships, including finding links that will stop IUU fishing and protect marine species.\nWe will address Question 1 of VAST challenge 2023 MC3 - To Use visual analytics to identify anomalies in the business groups present in the knowledge graph\n\n\nMethodology\nTo use different visualisation techniques to identify relationships and anomalies of entities in the businesss groups.\n\n\nData Preparation\n\nInstalling and loading of R packages\nThe code chunk below will be used to install and load the necessary R packages as follows:\n\njsonlite: a package supports both reading and writing JSON files, as well as working with JSON data retrieved from web APIs.\nigraph: a package in R that offers a wide range of tools for creating, manipulating, and visualizing graphs, as well as various algorithms and metrics for network analysis.\ntidygraph: provides a tidy and consistent approach to working with graph data using the principles of the tidyverse\nggraph: an extension package in R that builds upon the tidygraph and ggplot2 packages. It provides a high-level interface for creating visualizations of graph data using the grammar of graphics approach.\nviznetwork: a package provides geoms for ggplot2 to repel overlapping text labels.\ngraphlayouts: a package that provides a collection of layout algorithms that can be used to visualize graphs.\nggforce: extension package for ggplot2 that provides additional functionalities and extensions to enhance and expand the capabilities of ggplot2 for data visualization.\ntidyverse: a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\nscales: package that provides various functions for scaling and formatting data, primarily for data visualization purposes.\nwordcloud: package for creating word clouds, which are visual representations of textual data where the size of each word corresponds to its frequency or importance in the text.\ntm: text mining package that provides tools for text preprocessing, document-term matrix creation, and various other text mining tasks.\ntreemap: a package that allows you to create treemaps, which are visualizations that display hierarchical data as nested rectangles.\n\n\n\nCode\npacman::p_load(jsonlite, tidygraph, ggraph, \n               visNetwork, graphlayouts, ggforce, \n               skimr, tidytext, tidyverse, scales, wordcloud, tm, treemap)\n\n\n\n\n\n\nThe Data\nThe function fromJSON() of jsonlite package is used to import mc3.json into R environment. The data comprises:\n\nlist of 27622 “nodes” with 5 columns (id, country, type, revenue_omu, project_services)\nlist of 24038 “links” with 3 columns (source, target, type)\n\n\n\nCode\nmc3_data <- fromJSON(\"data/MC3.json\")\n\n\n\n\nData Preparation\nThe list of “links” are extracted to form an edge table. Correspondingly, the list of “nodes” are extracted to form a nodes table.\n\n\nCode\n#Extract edges from the list\nmc3_edges <- as_tibble(mc3_data$links) %>% \n  distinct() %>%\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type)) %>%\n  group_by(source, target, type) %>%\n    summarise(weights = n()) %>%\n  filter(source!=target) %>%\n  ungroup()\n\n\n\n\nCode\n#Extract nodes from the list\nmc3_nodes <- as_tibble(mc3_data$nodes) %>%\n  mutate(country = as.character(country),\n         id = as.character(id),\n         product_services = as.character(product_services),\n         revenue_omu = as.numeric(as.character(revenue_omu)),\n         type = as.character(type)) %>%\n  select(id, country, type, revenue_omu, product_services)\n\n\n\n\nData Exploration\n\nExploring edge table\nWe first explore the created edge table. The report below reveals that there are not missing values in all fields.We also made comparison between Different types in the edge table. The bar graph below indicates that there are 2 types of entities in the edge table - company contacts and owners.\n\n\nCode\nskim(mc3_edges)\n\n\n\nData summary\n\n\nName\nmc3_edges\n\n\nNumber of rows\n24036\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsource\n0\n1\n6\n700\n0\n12856\n0\n\n\ntarget\n0\n1\n6\n28\n0\n21265\n0\n\n\ntype\n0\n1\n16\n16\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nweights\n0\n1\n1\n0\n1\n1\n1\n1\n1\n▁▁▇▁▁\n\n\n\n\n\n\n\nCode\nggplot(data = mc3_edges, aes(x = type, fill = type)) +\n  geom_bar()\n\n\n\n\n\n\n\nExploring nodes table\nSimilarly, we explore the created nodes table. The report above reveals that there are 21515 missing values under “revenue_omu” column and these rows will be subsequently removed when visualising the revenues of the companies. The bar graph comparing the types of entities in the nodes table reveal that there are 3 types of entities - company, company contacts and owners.\n\n\nCode\nskim(mc3_nodes)\n\n\n\nData summary\n\n\nName\nmc3_nodes\n\n\nNumber of rows\n27622\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1\n6\n64\n0\n22929\n0\n\n\ncountry\n0\n1\n2\n15\n0\n100\n0\n\n\ntype\n0\n1\n7\n16\n0\n3\n0\n\n\nproduct_services\n0\n1\n4\n1737\n0\n3244\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrevenue_omu\n21515\n0.22\n1822155\n18184433\n3652.23\n7676.36\n16210.68\n48327.66\n310612303\n▇▁▁▁▁\n\n\n\n\n\n\n\nCode\nggplot(data = mc3_nodes, aes(x = type, fill = type)) +\n  geom_bar()\n\n\n\n\n\n\n\nData Wrangling\nWe would need to extract the embedded lists of entities found under the “Source” column in the original edges table, and itemise into individual rows. This is done by splitting the string in the embedded lists into edges sub-table, and removing whitespaces and irrelevant characters.\n\n\nCode\n#Extract lists from the edges table\nmc3_edge_unclean <- mc3_edges %>%\n  filter(substr(source,1,2) %in% \"c(\")\n\n\n\n\nCode\n# Break up the lists in the edge file by splitting the string\nmc3_edge_broken <- unnest(mc3_edge_unclean, source = strsplit(as.character(source), \"\\\\(|\\\\,|\\\\)\"))\n\n\n\n\nCode\n# Remove whitespaces amd filter records with \"c\" value\n#Create edges table incorporating the unlisted values\nmc3_edge_broken <- mc3_edge_broken %>%\n  mutate(source = gsub(\"\\\"\", \"\", source)) %>%\n  filter(source !=\"c\") %>%\n  mutate(source = trimws(source)) %>%\n  mutate(target = trimws(target)) %>%\n  group_by(source, target, type) %>%\n  summarise(weights = n()) %>%\n  filter(source != target) %>%\n  ungroup()\n\n\nFrom the original edges table, we will remove the lists, and subsequently concatenate the itemised values which were extracted from the embedded lists. This will form the clean edges table.\n\n\nCode\n#Create edges table without embedded lists from original edges table\nmc3_edges_without_list <- mc3_edges %>%\n  filter(!substr(source,1,2) %in% \"c(\") %>%\n  distinct()\n\n#Combine new edges table with edges table incorporated with the unlisted values, to form a combined edges table.  \nmc3_edges_combined <- rbind(mc3_edges_without_list, mc3_edge_broken)\n\n\nNext we will join the edges table with nodes table into combined edge table by mapping edge “source” column in edge table with “id” column in nodes table. This will provide us with the attribute details of the “source” entities in the edges table. Next, we assign “Company” type to the “source” entities on assumption that they are companies. The edges table will be used to derive the data for subsequent visualisation.\n\n\nCode\nmc3_edges_bysource <- left_join(mc3_edges_combined, mc3_nodes,\n                               by = c(\"source\" = \"id\"))\n\n\n\n#Assign source type as \"Company\"\nmc3_edges_bysource$type.y <- \"Company\"\n\n\n\n\nVisualisation\n\nCountries registered by top revenue companies\nThe treemap is constructed based on the top 50 companies that generates the highest revenues. It is observed that, of the top 50 companies with the highest revenues, more than half of the total revenues are earned by companies registered in ZH country. The top 3 highest earning companies are Jones LLC, Patton Ltd and Morgan Group. The diagram also revealed that Assam Limited Liability Company and Aqua Advancements Sashimi SE Express are the highest earning company in Utoporiana country and Oceanus country respectively.\n\n\nCode\n#Rename relevant columns as \"source type\" and \"target type\"\n#Filter combined edge table by removing \"unknown\" and \"character(0)\" values under \"Product_services\" column for subsequent relevant visualisation.\nmc3_edges_bysource1 <- mc3_edges_bysource %>%\n  group_by(source, target, type.y, type.x, country, weights, revenue_omu, product_services) %>%\n  filter(source!=target) %>%\n  rename(source_type = type.y) %>%\n  rename(target_type = type.x) %>%\n  filter(product_services != \"Unknown\") %>%\n  filter(product_services != \"character(0)\") %>%\n  ungroup()\n\n\n\n\nCode\n#Derive new edge table\nmc3_edges_toprevenue <- mc3_edges_bysource1 %>%\n  select (source, source_type, country, weights, revenue_omu, product_services) %>%\n  group_by(source) %>%\n  arrange(desc(revenue_omu)) %>%\n  distinct() %>%\n  ungroup()\n\n\n\n\nCode\n#Filter top 50 companies with highest revenues\nmc3_edges_toprevenue_filtered <- mc3_edges_toprevenue %>%\n  filter (source_type == \"Company\") %>%\n  slice_max(order_by = revenue_omu, n = 50)\n\n\n\n\nCode\n#Construct treemap\ntreemap(mc3_edges_toprevenue_filtered,\n        index=c(\"country\", \"source\"),\n        vSize=\"revenue_omu\",\n        vColor=\"revenue_omu\",\n        title=\"Revenue by Country and Company\",\n        title.legend = \"revenue of company\"\n        )\n\n\n\n\n\n\n\nIndividual owners with multiple companies\nOne of the anormalies that we can detect is the number of companies the individual entities own. We can be suspicious of these individuals if they own an exceptionally high number of companies.\nIn this visualisation on individuals who own more than 5 companies, we observe that the individual who owns the largest number of companies is John Smith (owns a total of 11 companies). This is followed by Micheal Johnson and Jennifer Smith who owns 9 and 8 companies respectively. We can also infer that this group of owners are dominated by the presence of the Smith family.\n\n\nCode\n#Filter individual owners from edge table, and assign target to be source and vice versa\nmc3_edges_indivowners <- mc3_edges_bysource %>%\n  select (target, source, type.x, weights) %>%\n  group_by(target) %>%\n  filter(type.x == \"Beneficial Owner\") %>%\n  rename (sc = target) %>%\n  rename (tg = source) %>%\n  distinct() %>%\n  ungroup()\n\n\n\n\nCode\n#Filter individuals who own more 5 companies\nmc3_edges_indivowners_filtered <- mc3_edges_indivowners %>%\n  group_by(sc) %>%\n  filter(n() > 5) %>%\n  ungroup()\n\n\n\n\nCode\n#Create corresponding nodes table out of the edges table\nid1_inv <- mc3_edges_indivowners_filtered %>%\n  select(sc) %>%\n  rename(id = sc)\nid2_inv <- mc3_edges_indivowners_filtered %>%\n  select(tg) %>%\n  rename(id = tg)\nmc3_nodes_indivowners_filtered <- rbind(id1_inv, id2_inv) %>%\n  distinct()\n\n\n\n\nCode\n#prep format for plotting\nmc3_edges_indivowners_filtered <- mc3_edges_indivowners_filtered %>%\nrename(from = sc) %>%\nrename(to = tg) %>%\n#filter (target_type == \"companies\") %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n#Plot interactive graph\nvisNetwork(mc3_nodes_indivowners_filtered, mc3_edges_indivowners_filtered) %>%\n  visNodes(color = list(background = \"pink\", border = \"red\")) %>%\n  visEdges(arrows = \"to\") %>%\n  visIgraphLayout(layout = \"layout_with_gem\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nCompanies that have registered in multiple countries\nWhile it is true that companies are typically registered in the country where they are headquartered or where they conduct their primary operations, it is not unheard of for companies to be registered in multiple countries. Nevertheless for our visualisation, we wish to identify such companies as such situations may not be entirely normal and these companies may potentially breach some form of legality.\nThe bar chart below shows the companies that have registered in more than one country. It reveals that Aqua Aura SE marine Life has the highest number of countries it registers in (9 countries in total). This is followed by Transit Limited Liability Company as well as Tamil Nadu S A/S, with both registered in 4 countries each.\n\n\nCode\n#Derive specific edge table and filter all companies\nmc3_edges_country <- mc3_edges_bysource %>%\n  select (source, country, type.y) %>%\n  group_by(source) %>%\n  filter(type.y == \"Company\") %>%\n  distinct() %>%\n  ungroup()\n\n\n\n\nCode\n#Filter companies that are registered in more than 1 country\nmc3_edges_country_filtered_one <- mc3_edges_country %>%\n  group_by(source) %>%\n  filter(n() > 1) %>%\n  ungroup()\n\n\n\n\nCode\nggplot(data = mc3_edges_country_filtered_one, aes(x = source)) +\n  geom_bar(fill = \"lightblue\") +\n  coord_flip() +\n  xlab(\"Companies\") +\n  ylab(\"No of countries registered\") +\n  scale_y_continuous(breaks = pretty_breaks(n = 5)) +\n  theme(axis.text.y = element_text(size = 4))\n\n\n\n\n\nTo gather deeper insights, we will drill down to more extreme cases to uncover deeper insights on such companies. The visualisation below focuses on companies which are registered in more than 2 countries. From the network plot, we can pick up the names of the countries that companies register in. In the case of Aqua Aura SE Marine Life, this company has registered in the 9 countries - Oceanus, Coralmarica, Alverossia, Nalakond, Rio Isla, Talandria, Icarnia, Mawazam and Isliandor.\n\n\nCode\n#Filter companies that are registered in more than 2 countries\nmc3_edges_country_filtered <- mc3_edges_country %>%\n  group_by(source) %>%\n  filter(n() > 2) %>%\n  ungroup()\n\n\n\n\nCode\n#Create corresponding nodes table out of the edges table\nid1_con <- mc3_edges_country_filtered %>%\n  select(source) %>%\n  rename(id = source)\nid2_con <- mc3_edges_country_filtered %>%\n  select(country) %>%\n  rename(id = country)\nmc3_nodes_country_filtered <- rbind(id1_con, id2_con) %>%\n  distinct()\n\n\n\n\nCode\n#prep format for plotting\nmc3_edges_country_filtered <- mc3_edges_country_filtered %>%\nrename(from = source) %>%\nrename(to = country) %>%\nfilter(from!=to) %>%\nungroup()\n\n\n\n\nCode\n#Plot interactive graph\nvisNetwork(mc3_nodes_country_filtered, mc3_edges_country_filtered) %>%\n  visNodes(color = list(background = \"lightgreen\", border = \"orange\")) %>%\n  visEdges(arrows = \"from\") %>%\n  visIgraphLayout(layout = \"layout_with_gem\") %>%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nWord cloud on product services\nThe word cloud provides an overview of the range of product services provided by the companies. It also provides us with an indication of the more common business groups and the product mix. We can infer that most of the companies are dealing in food related services, in particular seafood and fish. In terms of the fish types, the salmon would seem to be the most common.\n\n\nCode\ncolumn_values <- mc3_edges_toprevenue$product_services\n\n\n\n\nCode\n#Create a Corpus\ncorpus <- Corpus(VectorSource(column_values))\n\n#Preprocess the words\ncorpus <- tm_map(corpus, content_transformer(tolower))\ncorpus <- tm_map(corpus, removePunctuation)\ncorpus <- tm_map(corpus, removeNumbers)\n\n#Add custom stopwords\ncustom_stopwords <- c(\"products\", \"systems\", \"including\", \"solutions\", \"industrial\", \"wide\", \"short\", \"fat\", \"die\", \"related\", \"equipment\", \"range\", \"offers\", \"kits\", \"frozen\", \"wild\", \"kit\", \"soft\", \"hard\", \"non\", \"cooked\", \"ltl\", \"dust\", \"full\", \"processing\", \"high\", \"dried\", \"far\", \"low\", \"roll\", \"flat\", \"raw\", \"source\", \"lcl\", \"men\", \"strip\", \"include\", \"multi\", \"natural\", \"general\", \"unit\", \"care\", \"hot\", \"rare\", \"dry\", \"set\", \"provides\", \"involved\", \"ends\", \"chum\", \"freelance\", \"hvac\", \"etc\")\ncorpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\ncorpus <- tm_map(corpus, removeWords, custom_stopwords)\n\n#Create a term document matrix\ntdm <- TermDocumentMatrix(corpus)\n\n#Convert the term document matrix to a matrix\nm <- as.matrix(tdm)\n\n#Calculate word frequencies\nword_freq <- sort(rowSums(m), decreasing = TRUE)\n\n#Create the word cloud\nwordcloud(words = names(word_freq), freq = word_freq, scale = c(5, 1), random.order = FALSE, colors = brewer.pal(8, \"Dark2\"))"
  }
]